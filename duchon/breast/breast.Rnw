% investigating the breast cancer data
% David Lawrence Miller
% dave@ninepointeightone.net
  
\documentclass[a4paper,10pt]{article}
\setlength{\textheight}{22.5cm}
\setlength{\textwidth}{6.47in}
\setlength{\oddsidemargin}{-1mm}
\setlength{\topmargin}{0.1cm}
\setlength{\evensidemargin}{-5mm} 
 
% Load some packages
\usepackage{times, amsmath, amssymb, amsfonts, url, bm, rotating}
 
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{psfrag}
\usepackage{soul}

% top matter
\title{Breast cancer data analysis}
\author{David Lawrence Miller\\Mathematical Sciences\\University of Bath\\\texttt{dave@ninepointeightone.net}}
 
% Shortcuts
% Probability
\newcommand{\prob}[1]{\mathbb{P}\left[ #1 \right]}
% fprime
\newcommand{\fprime}{f^\prime(z)}
% figure reference command
\newcommand{\fig}[1]{\emph{fig.} \ref{#1}}
% Figure reference command
\newcommand{\Fig}[1]{\emph{Fig.} \ref{#1}}
% table reference command
\newcommand{\tabref}[1]{\emph{table} \ref{#1}}
% Table reference command
\newcommand{\Tabref}[1]{\emph{Table} \ref{#1}}
% equation reference command
\newcommand{\eqn}[1]{(\ref{#1})}
% phi inverse
\newcommand{\phiinv}{\phi^{-1}}
% use other phi
\renewcommand{\phi}{\varphi}
%transpose
\newcommand{\tr}[1]{#1^{\text{T}}}
% diagonal
\newcommand{\diag}{\text{diag}}
% call \times \cross
\newcommand{\cross}{\times}
% section references
\newcommand{\secref}[1]{section \ref{#1}}
\newcommand{\Secref}[1]{Section \ref{#1}}
\newcommand{\ijth}{$ij^{\text{th}}$}
% error function
\newcommand{\erf}{\text{Erf}}
% derivative shortcuts
\newcommand{\palphaj}{\frac{\partial}{\partial \alpha_{j*}}}
\newcommand{\pbetajk}{\frac{\partial}{\partial \beta_{j*k*}}}
%nu_j shortcut + starred
\newcommand{\nuj}{\nu_j(\bm{z}^{(j)})}
\newcommand{\nujs}{\nu_{j*}(\bm{z}^{(j*)})}
% z_ik^j shortcut + with starts
\newcommand{\zijk}{z_{ik}^{(j)}}
\newcommand{\zijkss}{z_{ik*}^{(j*)}}
% z^(j) shortcut + with starts
\newcommand{\zj}{\bm{z}^{(j)}}
\newcommand{\zjs}{\bm{z}^{(j*)}}
% z^(J) shortcut + with starts
\newcommand{\zJ}{\bm{z}^{(J)}}






\begin{document}

\maketitle

\section{Introduction}
Looking at the breast cancer data from Wit and McClure to see if the distances generated from microarray data can be used to smooth using Duchon splines and MDS. This is a Sweave document.

\section{Setting up the data...}

First off, load some packages....

<<>>=
library(mdspack)
library(ggplot2)
library(smida)
data(breast)
@

The data is in a list. Putting that into a \texttt{data.frame} by first taking the non-microarray data, which are:
\begin{itemize}
	\item \texttt{npi} - Nottingham prognostic index
	\item \texttt{surv.time} - surivival time
	\item \texttt{size} - size of the tumor (mm)
	\item \texttt{age.at.diag} - age at diagnosis
	\item \texttt{any.death} - whether the patient died
	\item \texttt{cancer.death} - whether that death was from cancer
	\item \texttt{cancer.grade} - cancer severity grade
\end{itemize}
Putting those in the frame \texttt{breast.dat}:
<<>>=
breast.dat<-as.data.frame(cbind(breast$npi,breast$surv.time, breast$size, breast$age.at.diag,
                                breast$any.death,breast$cancer.death,
                                breast$cancer.grade))
names(breast.dat)<-c("npi","surv.time","size","age.at.diag",
                     "any.death","cancer.death",
                     "cancer.grade")
@
And putting the microarray data in a matrix:
<<>>=
breast.array<-as.matrix(breast$dat)
@

\subsection{Response variables}

\subsubsection{NPI}

Can't look at the survival times as there are only 20 non-censored observations, instead let's try Nottingham prognostic index, NPI.
<<>>=
# save first
breast.array.full<-breast.array
breast.dat.full<-breast.dat

ind<-!is.na(breast$npi)
# array data
breast.array<-breast.array[ind,]
# other data
breast.dat<-breast.dat[ind,]
@
Excluding those observations which do not have NPI recorded we have 45 observations.

The Nottingham Prognostic Index is defined as:
\begin{equation}
\text{NPI} = 0.2(\text{size of index lesion in cm}) + \text{number of lymph nodes} + \text{tumour grade}
\end{equation}
see Haybittle \textit{et al.} (1982) and Todd \textit{et al.} (1987) for more information. High values of NPI are bad, identifying those patients with very poor prognoses.

\subsubsection{Tumour grade}

Can also look at the tumour grade. This was scored I-III (multinomial data) on a scale of decreasing cell differentiation (ie. large numbers are bad). The same observations had missing values for NPI as had missing values for tumour grade.

\section{EDA}

\subsection{NPI}

Looking at the effect that each variable has on NPI, just using a simple scatter plot, see fig \ref{npi-vs-plot}. Linear regression lines are superimposed. The following code generated the figure:
<<eval=false>>=
breast.m<-melt(breast.dat,c("npi","any.death","cancer.death"))
p<-ggplot(breast.m)
p<-p+geom_point(aes(value,npi))
p<-p+facet_wrap(~variable,nrow=1,scales="free_x")
p<-p+labs(x="", y="NPI")
 p<-p+geom_smooth(aes(value,npi),method="lm")
print(p)
@
From that figure, it's easy to see the relationship between NPI and its constituent parts.
a histogram of survival time is generated with the following code (and shown in fig \ref{npi-hist}):
<<eval=false>>=
p<-ggplot(breast.m)
p<-p+geom_histogram(aes(npi),binwidth=1)
p<-p+labs(y="Frequency", x="NPI")
print(p)
@

\begin{figure}[tb]
\begin{center}
<<fig=TRUE,echo=FALSE,cache=true>>=
breast.m<-melt(breast.dat,c("npi","any.death","cancer.death"))
p<-ggplot(breast.m)
p<-p+geom_point(aes(value,npi))
p<-p+facet_wrap(~variable,nrow=1,scales="free_x")
p<-p+labs(x="", y="NPI")
 p<-p+geom_smooth(aes(value,npi),method="lm")
print(p)
@
\end{center}
\label{npi-vs-plot}
\caption{Plotting Nottingham prognostic index against the other non-microarray data. Note that both size and cancer grade have obvious linear relationships with NPI, as they are used in its calculation.}
\end{figure}

\begin{figure}[tb]
\begin{center}
<<fig=TRUE,echo=FALSE,cache=true>>=
#p<-ggplot(breast.m)
#p<-p+geom_histogram(aes(npi),binwidth=1)
#p<-p+labs(y="Frequency", x="NPI")
#print(p)
hist(breast.m$npi,xlab="NPI",main="")
@
\end{center}
\label{npi-hist}
\caption{Histogram of Nottingham prognostic index.}
\end{figure}

\subsection{Tumour grade}

The histogram of tumour grade values is shown in figure \ref{grade-hist}.

%\begin{figure}[tb]
%\begin{center}
%<<fig=TRUE,echo=FALSE,cache=true>>=
<<echo=FALSE>>=
breast.m<-melt(breast.dat,c("cancer.grade","any.death","cancer.death"))
@
%p<-ggplot(breast.m)
%p<-p+geom_point(aes(value,cancer.grade))
%p<-p+facet_wrap(~variable,nrow=1,scales="free_x")
%p<-p+labs(x="", y="Tumour grade")
% p<-p+geom_smooth(aes(value,cancer.grade),method="lm")
%print(p)
%@
%\end{center}
%\label{grade-vs-all}
%\caption{Plots of tumour grade against the other}
%\end{figure}

\begin{figure}[tb]
\begin{center}
<<fig=TRUE,echo=FALSE,cache=true>>=
#p<-ggplot(breast.m)
#p<-p+geom_histogram(aes(cancer.grade),binwidth=1)
#p<-p+labs(y="Frequency", x="Tumour grade")
#print(p)
hist(breast.m$cancer.grade,xlab="Tumour grade",main="")
@
\end{center}
\label{grade-hist}
\caption{Histogram of tumour grade.}
\end{figure}

\section{Model development}

We would like to smooth over all the columns (gene expression profiles) in the microarray data. This leads to a model containing 59 covariates with only 45 observations. To further complicate matters, there may be a high degree of collinearity between genes. Tan \textit{et al.} (2005) suggest using a PCR-type method to select ``eigengenes'' in order to work around the collinearity present in the data.

MDS+DS can also be used in a similar way to get around these problems. MDS+DS uses distances calculated between observations' covariate values, those distances are then used to create a new data set using multidimensional scaling. This new data is in a high dimensional space (although not as high as the original data) so Duchon splines are used to reliably smooth in this new space.

Mathematically the model may be written as
\begin{equation*}
\text{response}_i = f(x_{1i}, \dots, x_{di}) + \epsilon_i, \qquad  \epsilon_i \sim D
\end{equation*}
where $D$ is some distribution and $x_{1i}, \dots, x_{di}$ is the MDS projection of datum $i$ into $d$ dimensional MDS space. $f$ is some smooth function. The response is either the tumour grade or NPI.

The dimension to use is chosen by optimising the GCV score, this is done by fitting the model in a number of different dimensions and choosing the dimensionality corresponding to the lowest GCV score. Note that there is no guarantee of the score monotonically decreasing as a function of dimension, so the score is calculated over all dimensions in a set range to ensure a minimum is found.

\subsubsection{Preparing the data}

Before doing the analysis we also drop those columns that contain \texttt{NA} values.
<<>>=
col.ind<-colSums(is.na(breast.array))>0
breast.array<-breast.array[,!col.ind]
@
This leaves a 45 by 27 matrix.

Calculating the distance matrix from this microarray data:
<<>>=
breast.dist<-dist(breast.array,diag=TRUE,upper=TRUE)
@

\subsection{NPI model}

\subsubsection{Model 1 -- simple}

Starting with the simplest possible MDS+DS model, using normal errors:
<<>>=
b.gcv<-gam.mds.fit(breast.dat$npi,breast.dist,NULL,45,c(2,0.85))
@
The arguments are as follows:
\begin{enumerate}
	\item \texttt{breast.dat\$npi} - the response variable (Nottingham prognostic index)
	\item \texttt{ddist} - the distance matrix (distances between patients)
	\item \texttt{NULL} - the MDS dimension to use, \texttt{NULL} indicates selection by GCV score
	\item \texttt{45} - the basis dimension to use
	\item \texttt{c(2,0.9)} - vector of the lower bound for the dimension and upper proportion of variability explained - used for the lower and upper bounds of the MDS dimension search
\end{enumerate}
at the moment we are not including age at diagnosis in the model.

The object returned is a list, one of its elements (\texttt{gcvs}) gives the GCV score for each dimension checked. This can be used to check for multiple minima in the score. Such a plot is shown in figure \ref{gcvscore}. The plot shows a clear minima when using a 5-dimensional projection of the data.

\begin{figure}[tb]
\begin{center}
<<fig=TRUE,echo=FALSE,cache=true>>=
plot(b.gcv$scores$dim, b.gcv$scores$score,ylab="GCV score",xlab="MDS dimension",type="l")
@
\end{center}
\label{gcvscore}
\caption{GCV score as a function of MDS projection dimension for model 1.}
\end{figure}

Performing model checking, we can first look at the results of a \texttt{summary} (the GAM object for the best model is contained in the \texttt{gam} element of the returned list):
<<>>=
summary(b.gcv$gam)
@
The $R^2$ is rather low and the EDF seems to indicate something strange going on. Using the ``natural'' parametrisation (see Wood (2006) p. 208--210), we can look the EDF of each term and see what's happening.
<<>>=
X.mm<-model.matrix(b.gcv$gam)
X.qr.R<-qr.R(qr(X.mm))

S<-b.gcv$gam$smooth[[1]]$S[[1]] # pull out penalty matrix
S<-cbind(rep(0,nrow(S)),S)
S<-rbind(rep(0,ncol(S)),S)


beta.dash<-X.qr.R%*%b.gcv$gam$coefficients
new.pen<-solve(t(X.qr.R))%*%S%*%solve(X.qr.R)
new.pen<-eigen(new.pen)

# the coefficients in their "natural" parametrisation
beta.dash.dash<-t(new.pen$vectors)%*%beta.dash

# so the new parameters are in beta.dash.dash
# the EDF of the ith parameter is
lambda<-b.gcv$gam$sp
EDF<-1/(1+lambda*new.pen$values)
@
Figure \ref{coeffplot1} shows the EDFs and coefficient values. It doesn't look like there is any smoothing going on here. The last terms with larger coefficients and EDFs are those terms in the nullspace of the penalty.

\begin{figure}[tb]
\begin{center}
<<fig=TRUE,echo=FALSE,cache=true>>=
plot(c(1,length(EDF)),c(min(EDF,as.vector(beta.dash.dash)),max(EDF,as.vector(beta.dash.dash))),type="n",xlab="Coefficient number",ylab="EDF/coefficent")
lines(1:length(EDF),EDF)
lines(1:length(as.vector(beta.dash.dash)),as.vector(beta.dash.dash),col="red")
@
\end{center}
\label{coeffplot1}
\caption{The coefficient values (red) and their corresponding EDFs (black) for model 1.}
\end{figure}

Figure \ref{gamcheck-1} shows that there are too many residuals in the lower tail of the histogram and slightly too few in the middle, this is reflected in the QQ plot. The right side of the plot doesn't suggest anything problematic, although it is rather difficult to see what is going on with such a small sample. 

The plot doesn't seem to indicate anything drastic going wrong, but the EDF calculations above show that we are not fitting much more than planes to the data.

\begin{figure}[tb]
\begin{center}
<<fig=TRUE,echo=FALSE,cache=true>>=
gam.check(b.gcv$gam)
@
\end{center}
\label{gamcheck-1}
\caption{Results of running \texttt{gam.check} on model 1.}
\end{figure}


\subsubsection{Model 2 -- Gamma errors}

Trying Gamma errors and an identity link provides a significant increase in the $R^2$.
<<>>=
b.gcv.gamma<-gam.mds.fit(breast.dat$npi,breast.dist,NULL,45,c(2,0.85),family=Gamma(link=identity))
summary(b.gcv.gamma$gam)
@
The results of running \texttt{gam.check} are shown in figure \ref{gamcheck-2}. The left side plots don't show that using Gamma errors offers an improvement.

\begin{figure}[tb]
\begin{center}
<<fig=TRUE,echo=FALSE,cache=true>>=
gam.check(b.gcv.gamma$gam)
@
\end{center}
\label{gamcheck-2}
\caption{Results of running \texttt{gam.check} on model 2.}
\end{figure}

\subsubsection{Model 3 -- quasi-likelihood}

Messing around with the quasi options gets:
<<>>=
b.gcv.quasi<-gam.mds.fit(breast.dat$npi,breast.dist,NULL,45,c(2,0.85),family=quasi(link=power(1/3),variance="mu^3"))
summary(b.gcv.quasi$gam)
@
Which gives a slightly improved $R^2$, GCV score and percent deviance explained. The histogram of residuals also looks a bit better.

\begin{figure}[tb]
\begin{center}
<<fig=TRUE,echo=FALSE,cache=true>>=
gam.check(b.gcv.quasi$gam)
@
\end{center}
\label{gamcheck-3}
\caption{Results of running \texttt{gam.check} on model 3.}
\end{figure}

Checking to see if there is any information left in the residuals, 
<<>>=
resid.data<-b.gcv.quasi$samp.mds
resid.data$response<-b.gcv.quasi$gam$residuals
b.gcv.resids<-gam(response~s(zw,zx,zy,zz,bs="ds",m=c(2,1.5),k=45),data=resid.data)
summary(b.gcv.resids)
@
There doesn't seem to be anything going on there.

\subsubsection{Assessment via leave-one-out cross-validation}
\label{npi-cv}

Can use leave-one-out cross-validation (LOOCV) to see how sensitive the results are to changes in the data.  To calculate the CV score, the following algorithm was used:
\begin{enumerate}
	\item Fit the model ($f^{(-i)}$, say) to the data with the $i^\text{th}$ datum removed.
	\item Predict over all the data.
	\item Calculate and record:
		\begin{equation}
		L(f^{(-i)}) = (\text{NPI}_i -f^{(-i)}(x_i))^2,
		\end{equation}
		(the difference between the left out observation and the model's prediction of that observation).
\end{enumerate}

The CV score can then be calculated as:
\begin{equation}
\text{CV}(f^{(-i)}) = \frac{1}{N} \sum_{i=1}^N L(f^{(-i)})= \frac{1}{N} \sum_{i=1}^N (\text{NPI}_i -f^{(-i)}.(x_i))^2
\end{equation}

Both the ``'best'' model (model 3) and the next best, (model 1) were compared with a Lasso (Friedman \textit{et al} (2010)), provided in the package \texttt{glmnet}. 

% see breast-cv-npi.R for the code for this!
The CV score for the lasso was 1.43, the normal and quasi-likelihood MDS+DS models scored 1.88 and 2.0 respectively. Looking at the EDFs for each LOOCV round, the normal model tended fit about half of the models as purely unpenalised functions (ie. just hyperplanes), as opposed to the quasi-likelihood model which only fit planes 3 times out of the 45 rounds. The MSE per round was roughly similar for models 1 and 3, with 3 doing slightly better on the larger deviations. Although models 1 and 3 had roughly similar MSEs in each round, the lasso tended to to not fail in the same way.

Figure \ref{mse-edf-npi-plot} shows the relationship between the MSE and EDF for the normal model per LOOCV round. As one can see from the plot, there is no strong relationship between the two, so it doesn't seem that the model is fitting an overly simplistic model and making very bad predictions with it consistently. In fact, it seems as though the simpler models (in EDF terms) are doing better in MSE terms (there are more points in the lower left of the plot).

\begin{figure}[tb]
\begin{center}
<<fig=TRUE,echo=FALSE,cache=true>>=
load("npi-edf-mse.RData")
plot(edf.mse.plot,xlab="EDF",ylab="MSE")
@
\end{center}
\label{mse-edf-npi-plot}
\caption{Plot of the EDF versus the MSE for the normal model for the NPI.}
\end{figure}

Also worth noting is that the most popular MDS projection dimension was 5, agreeing with that selected via GCV with the full data. Re-running the models with MDS projection dimension set to 5 (ie. no GCV search) yielded slightly better LOOCV scores for both models but lead to model 1 (with Normal errors) fitting more models with no penalised components.

Figure \ref{mse-compare-npi-plot} shows a plot of per-round MSE for the quasi model versus that of the normal model. This shows that the models score approximately the same per round.

\begin{figure}[tb]
\begin{center}
<<fig=TRUE,echo=FALSE,cache=true>>=
load("npi-cv-plotmse.RData")
plot(plot.mse,xlab="Quasi MSE per CV round", ylab="Gaussian MSE per CV round",asp=1)
abline(0,1)
@
\end{center}
\label{mse-compare-npi-plot}
\caption{Plot of per-round MSE for the quasi and gaussian models for the NPI. The points follow the line $y=x$, sgowing that the errors are broadly the same each round.}
\end{figure}

The quasi model is over-fitting slightly. However the Gaussian model is fitting overly simple models (those models with EDFs corresponding to fits involving only planes have consistently lower $R^2$ values, so there is no reason to believe they are fitting simpler models justifiably). There is not a huge difference in CV score between the two MDS+DS models. It therefore seems more reasonable to use the quasi model, even if it sometimes over-fits.


\subsection{Tumour grade model}

The tumour grade data are effectively multinomial, so need to use quasi-likelihood.
<<>>=
b.gcv.quasi<-gam.mds.fit(breast.dat$cancer.grade,breast.dist,NULL,45,c(2,0.85),family=quasi(link=identity,variance="constant"))
@
Using the method in the Wood (2006), pp. 231--232, we can look at the mean-variance relationship.
<<>>=
e<-b.gcv.quasi$gam$residuals
fv<-fitted(b.gcv.quasi$gam)
(lm(log(e^2)~log(fv))$coeff[2])^2
@
Trying that power and keeping the variance constant:
<<>>=
b.gcv.quasi<-gam.mds.fit(breast.dat$cancer.grade,breast.dist,NULL,45,c(2,0.85),family=quasi(link=power(0.4643001),variance="constant"))
summary(b.gcv.quasi$gam)
@
The results of running \texttt{gam.check} on this model can be found in figure \ref{grade-quasi-gamcheck} and the GCV score plot for finding the MDS projection dimension can be found in figure \ref{grade-gcv}.

\begin{figure}[tb]
\begin{center}
<<fig=TRUE,echo=FALSE,cache=true>>=
gam.check(b.gcv.quasi$gam)
@
\end{center}
\label{grade-quasi-gamcheck}
\caption{Results of running \texttt{gam.check} for the quasi-likelihood model for the tumour grade model.}
\end{figure}

Again, checking to see if there is any information left in the residuals:
<<>>=
resid.data<-b.gcv.quasi$samp.mds
resid.data$response<-b.gcv.quasi$gam$residuals
b.gcv.resids<-gam(response~s(zs,zt,zu,zv,zw,zx,zy,zz,bs="ds",m=c(2,1.5),k=45),data=resid.data)
summary(b.gcv.resids)
@



\begin{figure}[tb]
\begin{center}
<<fig=TRUE,echo=FALSE,cache=true>>=
plot(b.gcv.quasi$scores$dim, b.gcv.quasi$scores$score,ylab="GCV score",xlab="MDS dimension",type="l")
@
\end{center}
\label{grade-gcv}
\caption{Relationship between the GCV score and the MDS projection dimension for the quasi-likelihood model for the tumour grade model.}
\end{figure}

\subsubsection{Leave-one-out cross-validation}
% see breast-cv-grade.R for the code!
% also breast-cv-regtest-grade.R

As in section \ref{npi-cv} we can calculate the CV scores for the two models.

%% mean(link.pow)
%%[1] 0.703517
%%> median(link.pow)
%%[1] 0.6362234

Using the link power as determined above, MDS+DS yields a CV score of 0.88, while the lasso (using \texttt{glmnet} with multinomial errors) is slightly better with a score of 0.7. Changing the link power (for example running the regression to find a power per-LOOCV round) did not significantly effect the score.

Unlike the NPI model, all of the fitted models used a large number of unpenalised functions (the mean EDF being 33). As with the NPI model, the most popular MDS projection was  the same as the one selected by GCV for the whole data set (8).

\section{Concluding remarks}

It looks as though the MDS+DS approach can be used for microarray data and that this gives results not dissimilar to the lasso. It looks as though dimension selection (perhaps via variable selection before finding distances) is critical to creating robust models. Using LOOCV to determine the robustness of the models provides a useful way to check for over-fitting. Looking at the EDFs for each LOOCV round also shows when models are under-fitting.

Given how long the lasso has been in development, it is very encouraging that results from MDS+DS are rather close. Perhaps given further development the approach can begin to rival that of the lasso for applications where distance between observations rather than the actual value matters. 
 
\begin{thebibliography}{99}

\bibitem{} Friedman, J, T Hastie, and R Tibshirani. ``Regularization Paths for Generalized Linear Models via Coordinate Descent.'' \textit{Journal of Statistical Software} 33 (1) (2010): 1-22.

\bibitem{} Haybittle, JL, RW Blamey, and CW Elston. ``A prognostic index in primary breast cancer.'' \textit{British Journal of Cancer} (1982).
 
\bibitem{} Tan, Y, S Leming, W Tong, and C Wang. ``Multi-class cancer classification by total principal component regression (TPCR) using microarray gene expression data.'' \textit{Nucleic Acids Research} 33 (1) (2005): 56-65.

\bibitem{} Todd, JH, C Dowle, MR Williams, CW Elston, IO Ellis, CP Hinton, RW Blamey, and JL Haybittle. ``Confirmation of a prognostic index in primary breast cancer.'' \textit{British Journal of Cancer} 56 (4) (1987): 489-492.

\bibitem{} Wood, S. N. (2006). \textit{Generalized Additive Models: An Introduction with R}. London: Chapman \& Hall.

\end{thebibliography} 

\end{document}
