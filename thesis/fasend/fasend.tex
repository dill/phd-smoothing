\label{chap-fasend}

This chapter draws to a close the smoothing part of the thesis. It first compares the methods set forth here with the kriging methods alluded to in \secref{intro-leakageapproaches}, it then goes on to give information about the software implementation of the models discussed so far. Finally, the work from the last five chapters is summarized and further work proposed.

\section{Comparison with MDS-based kriging\\methods}
\label{gds-krig}

Kriging is focused on the explicit modelling of the correlations between points in space as a function of the distance between them via the estimation of the semivariogram. It is therefore logical that in, say, a river system the distances between points are calculated along the river's course rather than the Euclidean distance. 

For the semivariogram to be a valid covariance function, it must be positive definite or conditionally negative definite (see \citeb[p. 47]{diggle} for more information). However when non-Euclidean distances are used the semivariogram may no-longer fulfil either of these conditions (\cite{curriero}). The work of \citeb{mdskrig} attempts to solve this problem by using multidimensional scaling to project water distances into Euclidean space. Distances are not found exactly, a series of approximations are used rather than directly calculating the distances between the data. First the domain in question is triangulated, then the ``river distance'' between all of the nodes in the triangulation are calculated via Dijkstra's algorithm. The river distances are then projected using MDS. Finally, the data locations are mapped into the MDS space by interpolating between the grid points from the triangulation in MDS space.  The Euclidean distances in MDS space are then used in the estimation of the semivariogram. There are a number of issues with this approach. 

Most prominently, the authors only consider ordinary kriging (where the mean process is treated as constant). In this case spatial variation only enters the model through the semivariogram.The effect of using the MDS projected points for a spatially varying mean process (in addition to the estimation of the semivariogram) has not been investigated. Prevailing opinion is that only polynomial trends should be used for the mean process (\cite[p. 57]{diggle}), how such an approach would perform in higher dimensions is not clear.

Although the approximations used undoubtedly decrease the computational time, the validity of the approximations is not tested, especially on the fitting of the semivariogram (\cite{crabkrig}). The discretisation of the domain necessary to compute the graph distance via Dijsktra's algorithm has similar pitfalls to \citeb{wangranalli}. \citeb{crabkrig} suggest using the proportion of variation explained or the Bayesian criterion of \citeb{ohraftery} as possible metrics to perform projection dimension selection but do not full address the issue, resorting to 2-dimensional projections. Neither of the proposed selection methods take into account the effect that the dimension of the MDS projection has on the overall model (as discussed in \secref{gds-dimselect}).  \citeb{boisvert} suggest that to best approximate the distances, an $n-1$ dimensional projection of the distance matrix (if there are $n$ data, or triangulation nodes) be used (which is of course true) however they go on to point out that the use of such a high-dimensional projection could lead to numerical problems. Interpolating to find the distances in higher dimensions may also have its own issues and so the approximations may run into further problems. In all of these works the MDS projection is being used to approximate the within-area distances by a set of distances obeying the rules of a Euclidean metric (the criterion given by \citeb{curriero} to ensure valid semivariograms). Unlike in the material presented here, the MDS point configuration itself is not being used except to obtain a Euclidean approximation to the distances matrix so that the semivariogram can be estimated.

In general kriging methods suffer from having developed as an \textit{ad hoc} set of tools used in the mining industry (\cite[preface]{diggle}). Although much work has been done to improve the mathematical basis of kriging, models are not as flexible as GAMs, in particular the incorporation of other covariates, temporal effects and random effects is not straight forward as it is for additive and generalized additive models (in both theory and practice).

\section{Software implementation - \mdspack}
\label{gds-software}

The methods detailed in this first part of the thesis: the combination Duchon splines and MDS to perform smoothing (along with the within-area distance algorithm) are provided in the \textsf{R} package \mdspack\ (MultiDimensional Scaling for GAMs) which is available at \url{http://www.github.com/dill/msg}. Documentation is provided in the package and has been designed to be familiar to users of \texttt{mgcv} (\mdspack\ implements the methods detailed here as an extra basis for \texttt{mgcv} so minimal code changes are needed to try \mdsds\ on existing problems).

\section{Finite area smoothing conclusion}
\label{gds-conclusion}

This part of the thesis started by introducing additive and generalized additive models and the problem which arises when smoothing in a finite area when the boundary is a complex shape: leakage. Current approaches to the problem were then reviewed. Chapter \ref{chap-it} then illustrated the methods of the first chapter in practice. Based on the work in \citeb{miller2011}, the first application of the soap film smoother to model spatiotemporal data (via a tensor product formulation) was presented.

Chapters \ref{chap-sc}, \ref{chap-mds} and \ref{chap-gds} developed two transformation-based methods to combat leakage. At each stage of the work presented in this thesis the models became more refined and a more nuanced view of how the problem should be addressed was developed. Moving from a strictly functional mapping based on the boundary (the \sch\ transform) to one that preserves within-area distances (MDS) was key to finding a reliable transformation that avoided the artefacts caused by the squashing of space. The discovery that the projections produced by MDS can cause the ordering of the points to be lost in 2-dimensions explained the poor performance of the model up to that point. The final breakthrough was understanding that by projecting into higher dimensions, the ordering problem can be avoided and that by using Duchon splines reliable high dimensional can take place. This final model, \mdsds, performed very well in simulation, rivalling the soap film smoother.

As well as developing a method which is competitive with the current ``best'' (the soap film smoother), the investigations in the previous chapters have also revealed a set of essential and a set of desirable properties for transformation methods if they are to be used to perform spatial smoothing. The essential properties are:
\begin{enumerate}
\item The mapping of points must be smooth, there should be no sudden jumps or gaps. Points that are near one-another in the original space must be near one-another in the transformed space (\secref{sc-conclusions}).
\item The transformation must not squash space too much. Squashing points so they are numerically indistinguishable (crowding) must be absolutely avoided, but less severe compressions of space can also cause problems (\secref{sch-crowding} and \secref{mds-penadjust}).
\item Ordering of points must be maintained. If the response values are mis-ordered then modelling becomes impossible (\secref{pensuck}).
\end{enumerate}
As well as the above, there are other non-essential but desirable properties:
\begin{enumerate}
\item To make the method competitive in terms of computational time (chapter \ref{chap-it}), the mapping of points from the domain of interest into the transformed space must be fast. This can be achieved by using some kind of functional mapping (chapter \ref{chap-sc}) or by a sufficiently optimizing the procedure (\secref{mds-faster}).
\item Being able to integrate the spatial smooth into a larger model incorporating covariates, temporal interactions and random effects is extremely useful in practice (chapter \ref{chap-it} and \secref{gds-krig}).
\item Creating a method which appears to be familiar to the practitioner, will make the model building process much more streamlined. Combining this with a software implementation in a standard environment with a well-known paradigm can only help users (\secref{it-conc}).
\end{enumerate}

The methods proposed in chapters \ref{chap-sc}, \ref{chap-mds} and \ref{chap-gds} do not fully achieve all of these goals, however they achieve enough to be viable in practice. The lists above may be useful to those wishing to develop new methods based on transformations of space or further develop the methods presented here.

Moving on to further work, the speed of the algorithm for finding the within-area distances is still an issue, although there are several relatively simple tweaks that could help.

Sticking with the current algorithm, as seen in \secref{mdsdist}, finding schemes for the layout of starting grids and perhaps adapting the methods described in \citeb{mdskrig} to approximate the distances using a triangulation, would certainly increase performance (although perhaps at the price of accuracy).

As it stands distance generation is seen as a black box procedure to the model. This means that any procedure that can generate a distance matrix can be used. Aside from the discrete space approximation algorithms mentioned in \secref{mdsdist}, other measures can be used while still keeping in a roughly spatial context. One interesting approach would be to use distances in three dimensions, finding the shortest path over say a mountain range, which would include minimizing changes in altitude as well as avoiding obstactles. Alternatively, a cost based distance approach that takes into account fuel cost or taking into account difficult conditions (e.g. a bog or ford that can be crossed but at additional cost in terms of effort or time). One issue with such general cost-distance approach might be that the ``distance'' measure could turn out to be non-metric. That is, that the distance from A to B is not the same as the distance from B to A (for example going against versus going with the flow of a river). In this case non-metric MDS must be used, this relies only on the rankings of the data and discards other information, which is probably undesirable.

The examples presented here only consider smoothing inside of simple polygons since the within-area distance algorithm given in \secref{mdsdist} can only find shortest paths inside such shapes. This excludes domains with islands in them, which can occur in ship-board studies. This could be worked-around using other shortest path algorithms, however the behaviour of MDS (especially in higher dimensions) in such situations is unknown.

Even given its limitations, \mdsds\ does still show an improvement over the soap film smoother in MSE terms, as well as producing reasonable-looking maps in situations with real data. Only further testing on more data sets will show the limitations and the strengths of \mdsds, for now the method appears to be a useful addition to a practitioner's toolkit.

\section{Generalized distance smoothing conclusion}
\label{fasend-gds-conc}

The last chapter discussed smoothing in a more general setting, using MDS to project a matrix of dissimilarities that were not spatial in nature. Using general distances is appealing since most measurements have an arbitrary zero point (with the exception of some physical quantities like temperature). What is really of interest in most situations is the differences between the observations, and using these quantities directly makes sense. This is especially true in medical studies since there is evidence that those who are genetically similar are at risk of the same diseases, even if it is not known which genes in particular are indicators of the disease.

Despite the appeal of such a modelling strategy, problems arose. These centred on the choice of distance measure to be used. Choosing certain metrics gave better results than others, so the selection of the metric was down to trying many different options and seeing which was best (in the cases considered here, in terms of MSE, LOOCV or Brier score). The lack of any kind of continuum for the various possible distance measures means that a combination subject specific knowledge and trial and error must be used (rather than automation) to find the appropriate distance measure.

Even if a ``correct'' distance measure can be found, there is no guarantee that the resulting model will capture the key features of the data. This is due to the nature of MDS, as touched on in section \ref{gds-gds-examples}. MDS is based on taking the eigen-decomposition of the distance matrix, then using those eigenvectors with the largest eigenvalues to represent the points. Those directions with the largest eigenvalues are not necessarily those with the best predictive power. Using scores to select the number of dimensions overcomes this to some degree but because of the hierarchical nature of the projection only the number of dimensions can be selected. Of course, one could imagine the situation where the full MDS projection was found (in, say $n-1$ dimensions) and then variable selection could be performed on all of the possible combinations. This is not appealing if only because of the computational burden of performing the necessary subset selection. Using a full projection also rather subverts the point of the MDS, it would surely be easier to use a more traditional variable selection technique in that case.

In the examples in the previous chapter, the aim of using general distance smoothing has been to perform dimension reduction. However, in the finite area case the idea is to embed further information (about the boundary) into the distances, these two approaches are rather different. In the finite area case the MDS projected coordinates not only contain information about the position of the points in space in relation to one another, but also their position with respect to the boundary: the within-area distance algorithm and MDS procedure have imbued the projected coordinates with extra information. However, in the general distance case the idea is to discard the data which is not useful, a rather different objective and one that \mdsds\ does not appear to excel at.

\section{Conclusion}

This first part of the thesis has developed a transformation-based method for dealing with the problem of leakage in finite area smoothing. The physical model behind \mdsds\ is appealing since it does what one would intuitively want to do with a domain with a complex boundary: pull apart those areas of the domain that unduly influence one another. The final model presented in chapter \ref{chap-gds} performs at least as well in a spatial setting as the soap film smoother.

The key developments in this parts of the thesis are:
\begin{enumerate}
\item First application of the soap film smoother as part of a spatiotemporal model (chapter \ref{chap-it}).
\item Rejection of the \sch\ transform as a method for domain transformation, due to its propensity to overly squash together points in the resulting domain (chapter \ref{chap-sc}).
\item A new algorithm for finding distances between points in simple polygons (chapter \ref{chap-mds}).
\item Development of a domain transformation method to avoid leakage in finite area smoothing based on preserving within-area distances using multidimensional scaling. Subsequently that when using multidimensional scaling in this context that low-dimensional projections of points can cause a loss of order which is detrimental to smoothing (chapter \ref{chap-mds}).
\item Application of Duchon splines to avoid the problems associated with thin plate regression splines when performing high dimensional smoothing and use of GCV and REML scores to determine the necessary multidimensional scaling projection dimension in a spatial setting (chapter \ref{chap-gds}).
\item A general method for smoothing dissimilarities using multidimensional scaling to project the data (chapter \ref{chap-gds}).
\end{enumerate}

Further work includes adapting \mdsds\ to work with more complex domains (like non-simple polygons), applications to a wider set of domains and an investigation of utility of the method in larger models. The further development of the generalized distance smoothing ideas in the last chapter may prove extremely useful, provided that the issues surrounding the choice of metric can be addressed (particularly in a medical setting). For now it is hoped that \mdsds\ becomes a useful tool for those performing spatial modelling in complex domains.

