\label{chap-fasend}

This chapter draws to a close the smoothing part of the thesis. It first compares the methods set forth here with the kriging methods alluded to in \secref{intro-leakageapproaches}, going on to give information about the software implementation of the ideas discussed in this part. Finally, a summary of what has been done and further work that could be pursued in the area for both finite area and general distance smoothing.

\section{Comparison with MDS-based kriging methods}
\label{gds-krig}

Central to kriging is the explicit modelling of the semivariogram, relating the correlations between points in space according to the distance between the points. It is therefore logical that in (say) a river system, the distances through the river (i.e. the way the water travels) be used as the distances between points rather than the Euclidean distance. 

Measures such as river distances become problematic from a technical point of view since the semivariogram may then no-longer be positive definite or conditionally negative definite (\citeb{curriero}), which it must be to give a valid covariance function (see \cite[p. 47]{diggle} for more information).  \citeb{mdskrig} work around this by using multidimensional scaling to project water distances into Euclidean space. Distances are not found exactly, rather a series of approximations are used rather than directly calculating the distances between the data. First the domain in question is triangulated, then the ``river distance'' between all of the notes in the triangulation are calculated via Dijkstra's algorithm. Second, these distances are then projected using MDS. Finally, the data locations are mapped into the MDS space by interpolating between the grid points from the triangulation in MDS space.  The distances in MDS space are then used in the estimation of the semivariogram.

There are a number of issues with this approach as well as simple differences with the methods described in this thesis. Most prominently of all, only ordinary kriging (where the mean process is treated as constant) is considered by \cite{mdskrig}. In this case spatial variation only enters the model through the semivariogram.The effect of using the MDS projected points for a spatially varying mean process as well as for the distances used to estimate the semivariogram has not been explored. Prevailing opinion is that only polynomial trends should be used (\cite[p. 57]{diggle}), how such an approach would perform in higher dimensions is not clear.

Although the approximations used undoubtedly decrease the computational time, the validity of the approximations is not tested (especially on the fitting of the semivariogram, \citeb{crabkrig}). In particular, the use of the graph distance via Dijsktra's algorithm has similar pitfalls to \citeb{wangranalli} in the reliance on the discretisation of the domain and so may run into the issues discussed in \secref{grids} (although the issue of edges of the graph falling outside of the domain is avoided). Additionally, \citeb{crabkrig} do not full address the issue of the dimension of the MDS projection, merely suggesting the proportion of variation explained or the Bayesian criterion of \citeb{ohraftery} as possible metrics but resort to 2-dimensional projections. Neither of these methods take into account the effect that the MDS projection has on the overall model in terms of the response although this is not so problematic in the kriging case since only distances between points matter in the semivariogram, rather than connectivity. Using the interpolation in higher dimensions may also have it's own issues and so the approximations may run into further problems. \citeb{boisvert} suggest that to best approximate the distances the $n-1$ dimensional projection of the distance matrix (if there are $n$ data, or triangulation nodes) be used (which is of course true) however they go on to point out that the use of such a high-dimensional projection could lead to numerical issues. In all of these works the MDS projection is being used to approximate the within-area distances by a set of distances obeying the rules of a Euclidean metric (the criterion given by \citeb{curriero} to ensure valid semivariograms). 

In general kriging methods suffer from having developed as an \textit{ad hoc} set of tools used in the mining industry (\cite[preface]{diggle}). Although much work has been done to improve the mathematical basis of kriging, models are not as flexible as GAMs, in particular the incorporation of other covariates, temporal effects and random effects is not straight forward as it is for additive and generalized additive models (in both theory and practise).

\section{Software implementation - \mdspack}
\label{gds-software}

The methods detailed in this section, namely the use of Duchon splines along with MDS to perform smoothing are provided in the \textsf{R} package \mdspack\ (MultiDimensional Scaling for GAMs) which is available at \url{http://www.github.com/dill/msg}. Documentation is provided in the package and has been designed to be familiar to users of \texttt{mgcv} (\mdspack\ implements the methods detailed here as an extra basis for \texttt{mgcv} so minimal code changes are needed to use the method).




\section{Finite area smoothing conclusion}
\label{gds-conclusion}

At each stage of the work presented in this thesis the models became more refined and a more nuanced view of how the problem should be addressed was developed. Moving from a strictly functional mapping based on the boundary to one that relies on the shape of the domain was key to finding a reliable transformation that avoided the problem of crowding (moving from the \sch\ transform to MDS). The next big step was the realisation that the point configurations produced by MDS have problems with ordering and these can be avoided by projecting into higher dimensions. The final breakthrough was using Duchon splines to perform this high dimensional smoothing reliably.

The previous chapters have taken a simple idea (using domain morphing to combat leakage) and moved forward, along the way finding a set of properties for transformation methods that are necessary  if they are to be useful for spatial smoothing. These properties are:
\begin{enumerate}
\item The mapping of points must be smooth, there should be no sudden jumps or gaps. Points that are near one-another in the original space must be near one-another in the transformed space. See \secref{sc-conclusions}.
\item The transformation must not squash space too much. Squashing points so they are numerically indistinguishable (crowding) must be absolutely avoided, but less severe compressions of space can also cause problems. See \secref{sch-crowding} and \secref{mds-penadjust}.
\item Ordering of points must be maintained. If the response values are mis-ordered then the recovering of the truth becomes impossible (\secref{pensuck}).
\end{enumerate}
As well as the above, there are other non-essential but desirable properties:
\begin{enumerate}
\item To make the method competitive in terms of computational time, the method must be able to quickly map points from the data space into the smoothing space. This can be achieved by using some kind of functional mapping or by a sufficiently optimised way of finding within-area distances (\secref{mds-faster}).
\item Being able to integrate the spatial smooth into a larger model incorporating covariates, temporal interactions and random effects would be very useful.
\item The ability for the method to effectively be transparent to the practitioner is extremely useful, making the method look like something familiar and an implementation in existing software aids this.
\end{enumerate}

The methods proposed in chapters \ref{chap-sc} and \ref{chap-mds} do not fully achieve all of these goals, however the work in developing them did help to form the above lists. 

The speed of the algorithm for finding the within-area distances is still an issue, although there are several relatively simple tweaks that would make it faster. Finding schemes for the layout of starting grids and perhaps adapting the methods described in \citeb{mdskrig} to approximate the distances using a triangulation, would certainly increase performance (although perhaps at the price of accuracy).

The within-area distance algorithm given in \secref{mdsdist} can only find shortest paths for simple polygons. This excludes domains with islands in them, which obviously occur in ship-board studies. This could be worked-around using other shortest path algorithms, the behaviour of MDS (especially in higher dimensions) in such situations is unknown.

The advantage of seeing the distance generation as a black box procedure is that any procedure that can generate the necessary distance can be used. Aside from the discrete space approximation algorithms mentioned in \secref{mdsdist}, other measures can be used while still keeping in a roughly spatial context. One interesting approach might be to use distances in three dimensions, finding the shortest path over say a mountain range, which might be equivalent to minimizing changes in altitude. Alternatively, a cost based distance approach that takes into account fuel cost or taking into account difficult conditions (e.g. a bog or ford that can be crossed but at additional cost in terms of effort or time). These two extensions can be considered as roughly equivalent, since one could thinking of cost as an ``altitude'' that must also be minimized along with distance in the plane. One issue with such general cost-distance approach might be that the ``distance'' measure could turn out to be non-metric. That is, that the distance from A to B is not the same as the distance from B to A. In this case non-metric MDS must be used, this relies only on the rankings of the data and discards other information, which is probably undesirable.

Even given its limitations, \mdsds\ does still show an improvement over the soap film smoother in MSE terms, as well as producing reasonable-looking maps in situations with real data. Only further testing on more data sets will show the limitations and the strengths of \mdsds, for now the method appears to be a useful addition to a practitioners toolkit.

\section{Generalised distance smoothing conclusion}
\label{fasend-gds-conc}

Moving on to the more general distance smoothing presented in the last part of this chapter, in principle such an extension seems like the obvious next move. Using general distances is appealing since most measurements have an arbitrary zero point (with the exception of some physical quantities, e.g. temperature). So what is really of interest in most situations is the differences between the observations and using these quantities directly makes sense. This is especially true in medical studies since there is evidence that those who are genetically similar are at risk of the same diseases, even if it is not known which genes in particular are indicators of the disease.

However, the problems that were faced when trying to implement this were clustered around the problem of how to measure the distances. Choosing certain metrics gave better results than others, so the selection of the metric was down to trying many different options and seeing which was best (for some value of best). The lack of any kind of continuum for the various possible distance measures means that a combination subject specific knowledge and testing must be used (rather than automation) to find the appropriate distance measure.

Even if a ``correct'' distance measure can be found, there is no guarantee that the resulting model will capture the key parts of the data. This is due to the nature of MDS, as touched in section \ref{gds-gds-examples}. MDS is based on taking the eigen-decomposition of the distance matrix, then using those eigenvectors with the largest eigenvalues to represent the points. Those directions with the largest eigenvalues are not necessarily those with the best predictive power. Using scores to select the number of dimensions overcomes this to some degree but because of the hierarchical nature of the projection only the number of dimensions can be selected. Of course, one could imagine the situation where the full MDS projection was found (in, say $n-1$ dimensions) and then variable selection could be performed on all of the possible combinations. This is not appealing if only because the computational burden of performing the necessary subset selection. Using a full projection also rather subverts the point of the MDS, it would surely be easier to use a more traditional variable selection technique in that case.

The difference between the general distance case and the finite area case really is that in the general distance smoothing case the objective (as shown in the examples here, at least) has been dimension reduction and in the finite area case the idea is to embed further information into the distances, these are two rather different approaches. In the finite area case the MDS projected coordinates not only give information about the position of the points in space in relation to one another but also their position with respect to the boundary -- the within-area distance algorithm and MDS procedure has imbued the points with extra information. However, in the general distance case the idea is to discard the data which is not useful, a rather different objective and one that \mdsds\ does not appear to excel at.

Finally, the issue of interpretability arises. Even if \mdsds\ in the general distance setting were able to outperform the lasso in MSE terms, the model would be able to do only that. Interpretability is not \mdsds's strong suit, even plotting the results is difficult. It could however be used as in the finite area case, as a smoother to remove some kind of autocorrelation in the data, as a smooth that is not of main scientific interest but is never-the-less useful to the process of model building. Direct interpretation of the results smooths, however, is out of reach.

\section{Conclusion}

This part of the thesis began by reviewing the literature regarding finite area smoothing (\secref{intro-leakageapproaches}), moving on to a novel application of the current best methodology for finite area smoothing (soap film smoothing) to data from Italy (chapter \ref{chap-it}). From there two new methods based on the idea of morphing or warping the domain (via the \sch\ transform and multidimensional scaling) were developed to attempt to combat the phenomenon of leakage. The first method (chapter \ref{chap-sc}) had to be abandoned because of the limited set of domains that could be mapped to, however the lessons learnt from this gave many insights into how such techniques should work. The second method (chapters \ref{chap-mds} and \ref{chap-gds}), once combined with a suitable smoother, gave results that were on-par with, if not better than the current ``gold standard''. Although the method is not as fast as the soap film smoother, it does benefit from a simple setup and a relatively simple mathematical setup. Finally, from this a way of smoothing general distances or disparities via multidimensional scaling was developed (\secref{gds-gds-examples}) and even though the results presented here are not stellar, the potential and motivation behind such models is both interesting and compelling.

