% MDS stuff
\label{chap-mds}
\section{Introduction}

Following the work of the previous chapter, the objective is now to find a mapping of the data into a new space which causes minimum local distortion to the points in that new space, while still moving the points apart enough to avoid leakage. At the same time the mapping must also effectively separate those parts of the original domain which are subject to leakage. This balance (between making sure the model overcomes leakage whilst at the same time does not cause artefacts in the smooth) is essential to the success of any transformation-based approach. The \sch\ transform is clearly not flexible enough for general use since it dictates the transformed domain from the outset. A method that depends on the shape of original domain may have more promise.

\subsection{Proposition}

Multidimensional scaling (MDS) or, as it is often referred to, principal coordinates (PCO) (\cite{gower1966}) is a method commonly used in multivariate analysis. It is closely related to techniques such as principal\label{cor-r30} components analysis (PCA, \cite[p. 200]{chatfieldcollins}) and canonical correspondence analysis (CCA, \cite{terbraak}). The starting point for MDS is a matrix of distances, representing some kind of dissimilarity between observations. This distance could be calculated from the data, for example ideological distance between politicians measured using NOMINATE scores (\cite{quantss}, p. 225), or could instead be distances that occur in the data naturally through experimental setup,  for example comparative distances between stimuli response in a psychophysical experiment (\cite{torgerson}). The focus here is obviously on geographical distances.

MDS takes this matrix of distances and projects the data in such a way that Euclidean inter-point distances in the projection are approximately the same as the distances in the matrix (\cite[p. 187]{chatfieldcollins}). If the matrix of distances is of rank $n$ then the projection can be in $n-1$ or less dimensions; a projection into 2 dimensions is a typical choice, since it is easily visualised. For this reason one can also think of MDS as a dimension reduction technique, finding a projection of a data cloud into lower dimensional space, while still retaining information about the dissimilarities between the points.

When MDS is performed on some categorised set of dissimilarities (as is often the case in social science and psychology) it is referred to as non-metric MDS, where as on a continuous scale it is known as metric MDS. Discussion here will focus on metric MDS.

Multidimensional scaling provides a way to transform a domain in a similar way to \sch. Given the set of distances between points in a domain, we can project those points into a configuration such that the distances between those points are approximately preserved. Now, if the Euclidean metric were to be used to calculate the distances between the points then the result from the projection would be identical (up to rotation and translation) to the starting point configuration (provided that the projection had the same number of dimensions as the original data). However, if it were possible to use a metric that took into account the distance within the boundary (a \textit{within-area distance}) then the Euclidean distances between points in the resulting configuration would be (approximately) the same as the within-area distances. This would lead to distances used by the basis functions of the smoother to be approximately the within-area distances.

Justification for this approach is as follows. In many spatial applications within-area distances is meaningful, given that there is some reasoning behind why certain parts of the domain should not affect one another. Biological populations respect the intrinsic structure of these domains and in general do not respect Euclidean geometry in their movement patterns (for example, they move around obstacles, avoid predators and track prey). When within-area distances are meaningful, it makes sense to include the structure of the domain in the model, rather than somewhat arbitrarily choose Euclidean geometry and discard this extra information. However, as literature on smoothing is firmly based in a Euclidean context, it would be preferable to perform the smoothing in Euclidean space. In this case the approximation to Euclidean space afforded by an MDS projection of the within-area distances offers a bridge between these two requirements.

\subsection{Proposed procedure}
\label{mdsproc}

First take the sample locations $\mathbf{x}_i = (x_{1i}, x_{2i})$ (as in \secref{intro-basic-setup}) of the $i^\text{th}$ point with response $z_i$ (in general $\mathbf{x}_i$ could be $k$-vector, although 2-vectors of geographical coordinates are used throughout this chapter). The proposed procedure is as follows:
\begin{enumerate}
\item Obtain the MDS configuration for the domain using some representative set of points over the area in question. The only use of the MDS locations obtained in this step is to find the initial MDS configuration; they are discarded afterward. Representative points could be a sparse grid over the domain or a subset of $\{\mathbf{x}_i : i=1\dots n\}$. More detail and justification is provided in \secref{grids}, below.
\item Using the MDS configuration obtained above along with Gower's interpolation (see \secref{gowers}) to obtain the location of the sample in the MDS configuration: $\{\mathbf{x}_i^*, z_i : i=1\dots n\}$.
\item Smooth $\{\mathbf{x}_i^*, z_i : i=1\dots n\}$ using a penalised regression spline.
\item To predict at a location $\mathbf{x}_j$ in the original domain, use Gower's interpolation to obtain the point's location in the MDS space: $\mathbf{x}_j^*$. Predict $\hat{f}(\mathbf{x}_j)$.
\end{enumerate}
Here, finding an MDS configuration of a set of points consists of: $i$) calculating the within-area distances between the points, $ii$) forming the distance matrix and, $iii$) actually performing the MDS projection; the details of each step are covered in the following sections. This approach is referred to as \mdsap\ (MultiDimensional Scaling with Regression Splines) throughout the chapter.

The rest of this chapter is structured as follows: in \secref{MDStechdet} a technical overview of MDS is given, along with technical details of how the MDS configuration is calculated; \secref{mdsdist} focuses on how the within-area distances are found; \secref{mdssims} shows some examples of this method on simulated data. Sections \ref{mds-faster} and \ref{mds-penadjust} show some improvements to the initial method and further simulations, \secref{mds-problems} details remaining problems. Finally, \secref{mds-conc} draws the chapter to a conclusion and lays out areas of further work for the next chapter.


\section{Technical details}
\label{MDStechdet}

The basic concept behind MDS as used here is to take the data, calculate their within-area inter-point distances and then find their locations in a new coordinate system based on those inter-point distances. Their new locations are determined by finding the eigen-decomposition of the (centred) matrix of distances between points. First a description of the MDS procedure when Euclidean distances are used is given, followed by the justification for the use of the same procedure when using within-area distances. 

\subsection{Finding the new point configuration}

First define $d_{ij}$ as the distance between the points $i$ and $j$. These are used to form a symmetric $n \cross n$ matrix, $\mathbf{D}$, with \ijth element $d^2_{ij}$. For the moment let us ignore the issue of how distances are calculated and assume that $d_{ij}$ is simply the Euclidean distance between points $i$ and $j$. 

\citeb{diaconis08} gives a clear definition of the algorithm (due to \cite{schoenberg35} and \cite{torgerson}) for finding the new locations of points, which is outlined below. Further detail is given in \citeb{principlesofMA}, pp. 104--108 and \citeb{chatfieldcollins}, pp. 189--200.


\label{cor-r31}First suppose that the original data locations (the $\mathbf{x}_i$s) are unknown, let us find a spatial configuration which reproduces the same distance matrix as the $\mathbf{x}_i$s, then show that there is a simple relation between these two configurations. The $n$ locations in MDS space, $\mathbf{x}^*_i$ (for $i=1,\dots,n$), form the rows of an $n \times p$ matrix, $\tilde{\mathbf{X}}^*$ (these new locations reside in $p$-dimensional space, which we can find lower dimensional projections of; see below). Now let $\mathbb{S}=\tilde{\mathbf{X}}^{*} \tilde{\mathbf{X}}^{*\text{T}} $, so $\mathbb{S}$ is a matrix of scalar products of the point vectors, i.e. the \ijth element of $\mathbb{S}$ is:
\begin{equation}
\mathbb{S}_{ij} = \mathbf{x}_i^* \left ( \mathbf{x}_j^* \right)^\text{T},
\label{selem}
\end{equation}
where $\mathbb{S}$ is an $(n\cross n)$ matrix and $\mathbf{x}_i^*$ is as above. Note that $\tilde{\mathbf{X}}^*$ may only be found up to a translation and rotation (this does not alter the Euclidean distances). The centroid of the points in $\tilde{\mathbf{X}}^*$ is at the origin, so the sum of each column of $\tilde{\mathbf{X}}^*$ is zero.\label{cor-4s1}

We now wish to relate $\mathbf{D}$ to $\mathbb{S}$. First, note that that \ijth element of $\mathbf{D}$ is 
\begin{equation}
d_{ij}^2 = (\mathbf{x}^*_i-\mathbf{x}^*_j)\tr{(\mathbf{x}^*_i-\mathbf{x}^*_j)} = \mathbf{x}^*_i \left(\mathbf{x}^*_i\right )^\text{T} + \mathbf{x}^*_j \left ( \mathbf{x}^*_j\right )^\text{T}  -2 \mathbf{x}^*_i \left (\mathbf{x}^*_j\right )^\text{T}.
\label{dij}
\end{equation}
Using (\ref{selem}),  (\ref{dij}) can be written as:
\begin{equation}
\mathbf{D}=\text{diag}\left(\mathbb{S}\right)\tr{\bm{1}} + \bm{1}\tr{\text{diag}\left(\mathbb{S}\right)} -2\mathbb{S},
\label{dijmat}
\end{equation}
where $\bm{1}$ is an $n \cross 1$ vector of 1s and $\text{diag}\left(\mathbb{S}\right)$ is the $n \cross 1$ vector of diagonal elements of $\mathbb{S}$.

Define:
\begin{equation}
\mathbf{H} = \mathbf{I}-\frac{1}{n}\bm{1}\tr{\bm{1}},
\end{equation}
where $\mathbf{I}$ an $n \cross n$ identity matrix and $\bm{1}\tr{\bm{1}}$ is an $n \cross n$ matrix of 1s.

By pre- and post-multiplying any matrix by $\mathbf{H}$ the matrix is double centred (such that row and column means are 0). Pre- and post-multiplying \eqn{dijmat} by $\mathbf{H}$ yields:
\begin{equation}
\mathbf{H}\mathbf{D}\mathbf{H} = -2\mathbf{H}\mathbb{S}\mathbf{H}.
\end{equation}
\label{cor-r31-1}The contributions from the first two terms on the right hand side of \eqn{dijmat} are zero since the rows of $\text{diag}\left(\mathbb{S}\right)\tr{\bm{1}}$ and the columns of  $\bm{1}\tr{\text{diag}\left(\mathbb{S}\right)}$ are constant. Since $\mathbb{S}$ is already centred so $\mathbf{H}\mathbb{S}\mathbf{H}=\mathbb{S}$. Rearranging, the following relation between $\mathbb{S}$ and $\mathbf{D}$ holds:
\begin{equation}
\mathbb{S} = -\frac{1}{2}\mathbf{H}\mathbf{D}\mathbf{H}.
\end{equation}

\label{cor-r31-2}Having found a relation between $\mathbf{D}$ and $\mathbb{S}$, we can now find the relation between $\mathbb{S}$ and $\tilde{\mathbf{X}}^{*}$. This is simply a case of finding the eigen-decomposition of $\mathbb{S}$. The eigen-decompostion is useful since we wish to decompose the space based on the directions of largest variation (those that contribute to $\mathbb{S}_{ij}$ the most).

Finding the eigen-decomposition of $\mathbb{S}$, we obtain $\mathbb{S}=\mathbf{U}\mathbf{\Lambda}\tr{\mathbf{U}}$. Here $\mathbf{U}$ is the $n \cross n$ matrix with orthogonal columns which are the eigenvectors of $\mathbb{S}$ and $\mathbf{\Lambda}$ is the $n \cross n$ diagonal matrix of eigenvalues of $\mathbb{S}$ (in descending absolute value). An $\tilde{\mathbf{X}}^*$ satisfying (\ref{selem}) may then be computed as:
\begin{equation}
\tilde{\mathbf{X}}^*=\mathbf{U}\mathbf{\Lambda}^{\frac{1}{2}}.
\end{equation}
To summarize the above procedure, given that we have a distance matrix $\mathbf{D}$, the following steps can be performed to find the MDS projection of the data that $\mathbf{D}$ was calculated from:
\begin{enumerate}
\item Finding the matrix $\mathbb{S}=\frac{1}{2}\mathbf{H}\mathbf{D}\mathbf{H}.$
\item Eigen-decomposing $\mathbb{S}$, to obtain $\mathbb{S}=\mathbf{U}\mathbf{\Lambda} \tr{\mathbf{U}}$.
\item Computing $\tilde{\mathbf{X}}^*=\mathbf{U}\mathbf{\Lambda}^{\frac{1}{2}}$.
\end{enumerate}

The aim here is to smooth in two dimensions, and in general multidimensional scaling is performed to reduce the dimensionality of the data, so we must now reduce the dimensionality of $\tilde{\mathbf{X}}^*$. To represent the space using two dimensions the directions with the two largest eigenvalues are chosen and the others discarded. These two largest eigenvalues and their associated eigenvectors constitute the two largest sources of variation in distance (since they are the two largest contributions to $\mathbb{S}$) this gives the two dimensional representation of the data. 

Defining $\mathbf{X}^*$ to be the $n \cross 2$ dimensional matrix found by truncating $\tilde{\mathbf{X}}^*$ to its first two columns, a 2-dimensional representation has been obtained. More generally, the $k$-dimensional MDS representation of the space can be found by taking the first $k$ columns of $\tilde{\mathbf{X}}^*$.

In summary, to calculate the MDS configuration of a set of points (given their inter-point distances) we merely need to double centre the matrix of distances, perform an eigen-decomposition on the resulting matrix, and finally truncate the eigen-decomposition and find the new point set.

For finite area smoothing, $d_{ij}$ should be the shortest distance between the points $i$ and $j$ such that the path remains within the domain. Calculation of the inter-point distances is covered in \secref{mdsdist}, however it is important to first justify the use of these steps when non-Euclidean distances are used. Given that the distances in $\mathbf{D}$ obey the triangle inequality, the $n$ points from which $\mathbf{D}$ is computed may be represented by MDS in \label{cor-e8}$(n-1)$-dimensional Euclidean space (at worst, see \citeb{gower1968}\label{cor-e9}). \label{cor-r31-3}In the case when the distances in $\mathbf{D}$ are shortest within-area distances, one can think of the points as residing in a higher number of dimensions in such a way that the distances between them are Euclidean.

\label{cor-r31-4}
%Note that an additive constant can be computed and added to the non-diagonal entries of $\mathbf{D}$ to ensure that the eigenvalues of $\mathbb{S}$ are non-negative. However, this does not occur in any of the examples shown here.

\subsection{Gower's interpolation} 
\label{gowers}
Given the setup in \secref{mdsproc}, once the MDS configuration has been found further points will need to be inserted into our MDS representation. For example when further data is collected, or in order to predict over points not in the initial grid. In this case we would like to insert those new points into the configuration given by MDS. This can be performed by Gower's interpolation (\cite{gower1968}).

Say we have some point, $x_{\text{new}}$ and we wish to find its location, $x^*_{\text{new}}$ in the current MDS configuration. The position of $x^*_{\text{new}}$ is at a Euclidean distance from the points in $\mathbf{X}^*$ which is approximately the same as the (within-area) distance between $x_{\text{new}}$ and the points in the non-transformed space, $\mathbf{X}$. 

Note that here it is assumed that a 2-dimensional projection has been used in the initial MDS configuration, Gower's interpolation remains valid for the case in which the initial MDS projection is $k$-dimensional.

\subsubsection{Gower's interpolation formula}

We may find the position in the transformed space, $x^*_{\text{new}}$ , of some new datum $x_{\text{new}}$ in the original space using:
\begin{equation}
\tilde{x}^*_{\text{new}} = \frac{1}{2} \mathbf{\Lambda}^{-1} \tr{(\tilde{\mathbf{X}}^*)} \mathbb{D}.
\label{gower}
\end{equation}
Here $\mathbf{\Lambda}$ ($n \cross n$) and $\tilde{\mathbf{X}}^*$ ($n \cross p$)\label{cor-r31-5} are as above, $\mathbb{D}$ ($n \cross 1$) is the vector of centred within-area distances from the points in the original configuration to $x_{\text{new}}$. The resulting vector, $\tilde{x}^*_{\text{new}}$, is a $p$-vector, so again we can truncate to a $k$-vector in a similar way to above (taking the first two entries gives the 2-dimensional projection).

% Thesis fact: this paragraph infact represents about 3 months of work :(
In \citeb{gower1968} the $i^\text{th}$ element of $\mathbb{D}$ is defined as $-(d^2_{i,\text{new}}-\text{diag}(\mathbf{\tilde{X}}^* \mathbf{\tilde{X}}^{*\text{T}})_i)$\label{cor-4s2}, with $d^2_{i,\text{new}}$ being the squared distance from the $i^\text{th}$ point to the new point. The centring is given by the diagonal elements of $\tilde{\mathbf{X}}^*\tilde{\mathbf{X}^{* \text{T}}}$ i.e. the squared distances from the original points to the centroid of the MDS configuration. To avoid confusion (and to emphasise that the full $\tilde{\mathbf{X}}^*$ matrix is used, rather than its truncated version), it may be easier to think of the expression for the $i^\text{th}$ element of $\mathbb{D}$ as $-(d^2_{i,n+1}-\text{diag}(\mathbb{S})_i)$\label{cor-r31-5-2}. Since $\mathbb{S}$ is already known, this expression is more sensible to use for computation as it doesn't imply any extra matrix multiplication.

Gower's interpolation extends simply to the case when $m$ new points are inserted by having $\mathbb{D}$ as an $n \cross m$ matrix (so that the resulting $\tilde{x}^*_{\text{new}}$ is an $p\cross m$ matrix that can be truncated to the first $k$ columns, as above).


%\subsection{Practical considerations}
\subsection{Using grids to compute the initial MDS configuration}

\citeb{gower1968} shows that performing MDS on a dataset is equivalent to performing MDS on a reduced set of points and then inserting the remaining points when the Euclidean metric is used to calculate the distances\label{cor-r31-6}. However in the within-area distance case there can the potential problems if the reduced set of points does not encapsulate enough information about the domain. The first section here addresses this problem. The second section investigates the new configuration of points \label{cor-r31-7}in a similar way to check for the problematic squashing of space seen when the \sch\ transform was used in the previous chapter.

%\subsubsection{Using grids to compute the initial MDS configuration}
\label{grids}

The only place that the boundary enters the model is through the MDS configuration and in turn, the MDS configuration's only influence from the boundary is via the distances in $\mathbf{D}$. In a spatial setting one can imagine the case in which samples were not taken from one part of the domain of interest (for example, there may be no observations in a particular peninsulae) and in that case the resulting MDS configuration would differ from a configuration when data from the whole domain was used. Ensuring that different analyses on the same domain of interest yield consistent results is extremely important.

When Euclidean distances are used to calculate $\mathbf{D}$, the criterion needed so that the resulting space is the same (in the sense given above, that insertion into a reduced point set is the same as mapping the complete point set) is that there is one more point used to create the MDS configuration than there are dimensions in the space in which they reside (provided the points are not collinear) (\cite{landmark}). There is no similar criteria for within-area distances and it is unclear what form such a criterion would take.

A simple example of this problem is shown\label{cor-r32} in \fig{tshape}. Here, a regular grid has been generated inside a T-shape (top left panel). The point configuration found by using the full set of points and within-area distances is given in the top right panel. Sampling only the ``head'' or ``tail'' of the T and using those points to generate the MDS  configuration, then inserting the other points leads to the plots in the bottom left for head and bottom right respectively (red points are inserted into the black configuration). One can see that the configuration of the inserted points looks warped compared to the configuration in the top right. 

% showing the the grid is necessary using the T shape
\begin{figure}
\centering
% trim order l b r t
\includegraphics[width=6in]{mds/figs/tshape.pdf} \\
\caption{Data generated inside a T-shape (top left) is fed into MDS at once (top right). When either the head or tail of the T is used for the original MDS configuration and the other points inserted, the shape produced is distorted (bottom row). In the bottom row the black points give the initial configuration and the red points are those inserted later.}
\label{tshape}
% generated using figs/gridtest.R
\end{figure}

Although the cases shown in \fig{tshape} are somewhat pathological, looking at more reasonable situations still shows that the results can vary a considerable amount. In \fig{tshaperand} the black and green points make up the original MDS configuration; the five green points are chosen at random. The red points are then inserted. As can be seen in these four typical realisations, the shape of the MDS space is dependent on those points used to create the initial MDS configuration.

% showing the the grid is necessary using the T shape (random samples
\begin{figure}
\centering
% trim order l b r t
\includegraphics[width=6in]{mds/figs/tshaperand.pdf} \\
\caption{Using the T-shape in \fig{tshape} (top left), the tail (black points) of the T was used with 5 randomly sampled (green) points in the head. The head (without the 5 green points) was then inserted into the MDS configuration (red). As can be seen from these four realisations, the output varies greatly depending on the points sampled.}
\label{tshaperand}
% generated using figs/gridtest.R
\end{figure}

This problem can be rectified by using an appropriately spaced grid over the domain to calculate the eigen-decomposition, thus ensuring that the whole domain is covered. Provided that the grid is fine enough to catch all of the important features in the boundary of the domain, the problems above should not arise.

%\subsubsection{Point density and distribution in MDS space}
%\label{mds-smoothness}
%
%Given that the \sch\ transform presented issues with regard to the squashing of the points, before starting to perform smoothing it is useful to look at what configurations of points produced by using MDS with within-area distances look like.
%
%Taking the evenly spaced 50 by 50 point grid in \fig{wt2-grid-orig}, first MDS is performed on a dense point set of size 1253, and then a less dense grid is inserted using Gower's interpolation. The grid produced under the insertion can be seen in \fig{wt2-grid-full}. Taking a sample of 250 points from the 1253, an MDS configuration was also found and the same grid inserted (see \fig{wt2-grid-samp}). From this it is clear that those points mapped into the domain are smooth but in the sample case the features in the far right of the shape (the less pronounced peninsulae) are slightly squashed.
%
%% grid to map
%\begin{figure}
%!TEX encoding = UTF-8 Unicode\centering
%% trim order l b r t
%\includegraphics[width=4in]{mds/figs/wt2-grid-orig.pdf} \\
%\caption{The grid to be inserted into the MDS configuration over the peninsula domain to test the smoothness of the mapping.}
%\label{wt2-grid-orig}
%% generated using wt2-grid.R
%\end{figure}
%
%% mapped grid (full)
%\begin{figure}
%\centering
%% trim order l b r t
%\includegraphics[width=5in]{mds/figs/wt2-grid-full.pdf} \\
%\caption{Inserted grid when 1253 points are used to create the initial MDS configuration. The right panel shows a zoom of the far right part of the configuration.}
%\label{wt2-grid-full}
%% generated using wt2-grid.R
%\end{figure}
%
%% mapped grid (samp)
%\begin{figure}
%\centering
%% trim order l b r t
%\includegraphics[width=5in]{mds/figs/wt2-grid-samp.pdf} \\
%\caption{Inserted grid when 250 randomly chosen points are used to create the initial MDS configuration. The right panel shows a zoom of the far right part of the configuration. Comparing this to that of \fig{wt2-grid-full}, one can see that the features on the right have been slightly squashed together.}
%\label{wt2-grid-samp}
%% generated using wt2-grid.R
%\end{figure}
%
%In conclusion, the mapping can be both reliable and also produces a smooth configuration of points provided that the initial MDS configuration covers the space in sufficient detail.


\section{Finding the within-area distances}
\label{mdsdist}

So far it has been assumed that the matrix of distances $D$ is known. Using Euclidean distances would lead to the original configuration of points being recovered (up to translation and rotation) provided the projection dimension was the same as that of the original set of points. As was pointed out in \secref{intro-FAS}, Euclidean distances are the cause of leakage in the first place, since they do not reflect the distances that must be travelled between points residing in the domain of interest. This section describes (what the author believes to be) \label{cor-r33}a novel algorithm to find shortest paths within a given domain.

Note that paths between point pairs in \textit{simple} polygons (i.e. those polygons without holes) are considered. Although this limits the types of domains that can be addressed, it does make the shortest path algorithm simpler, since the shortest path is unique.

There are several methods available to calculate within-area distances, however given that the domain of interest is any arbitrary simple polygon this limits the number that are applicable. The two most promising possibilities are the geodesic methods used by \citeb{wangranalli} (see also \secref{intro-leakageapproaches}) and the $\text{A}^*$ algorithm (\cite{astarpaper}) which can be thought of as a generalisation of Dijkstra's algorithm (\cite{dijkstra}). However, both of these algorithms rely on the discretization of the domain of interest. As stated in \secref{intro-leakageapproaches}, this discretization of the domain is undesirable since the results then become dependent on the resolution of the discretization of the domain, even if a high enough resolution can be used the computational cost becomes prohibitively expensive for such methods. It would be preferable to have an elementary algorithm (i.e. one not relying on complex data structures or extensive theory) for finding the shortest path within the polygon.

The algorithm is defined as follows, it may be helpful to look at the example in \fig{wdia} while reading.

Let the domain boundary be some polygon, $\Gamma$. Given that there is no direct path within the domain between two points ($p_1$ and $p_2$, say), the algorithm proceeds as follows to create a path, $\mathcal{P}$, which is an ordered set of vertices:
\begin{enumerate}
\item (INIT) Start by drawing a line between $p_1$ and $p_2$ (\fig{wdia}, ($i$)). Start the path as the lines from $p_1$, $p_2$ to their nearest intersection with the boundary of $\Gamma$ ($p_1^1$, $p_2^1$, say). Then form two paths. The first path from $p_1^1$ to $p_2^1$ ($\mathcal{P}_1$) contains the vertices of $\Gamma$ found moving along the boundary from $p_1^1$ to $p_2^1$. The second ($\mathcal{P}_2$), is found by taking the path from $p_1^1$ to $p_2^1$ in the other direction around the boundary, ie. the vertices of $\Gamma$ not in the first path. It is easy to see that $\{\mathcal{P}_1 \cup \mathcal{P}_2\} \setminus \{p_1^1, p_2^1\} = \Gamma$. The DELETE step (below) is then performed on $\mathcal{P}_1$ and $\mathcal{P}_2$, removing any superfluous vertices. Finding the length of $\mathcal{P}_1$ and $\mathcal{P}_2$ and choosing the shorter ($\mathcal{P^*}$), the initial path is formed as $\mathcal{P}=(p_1,p_1^1,\mathcal{P}^*,p_2^1,p_2)$. 

In \fig{wdia}, ($iii$), $\mathcal{P}_1$ is marked in green and is chosen to form the initial path, $\mathcal{P}=(p_1,p_1^1,\mathcal{P}_1,p_2^1,p_2)$, as $\mathcal{P}_1$ is shorter than $\mathcal{P}_2$, in red.

\item (DELETE) Given a triple of vertices, $(v_i, v_{i+1}, v_{i+2}) \in \mathcal{P}$ , if the line between $v_i$ and $v_{i+2}$ is shorter than the path $(v_i, v_{i+1}, v_{i+2})$ and the line between $v_i$ and $v_{i+2}$ lies inside $\Gamma$ then delete $v_{i+1}$ (\fig{wdia}, ($iv$) and ($vi$)). The entire path is iterated over ($i=1,\ldots,N-2$, if there are $N$ vertices in $\mathcal{P}$)  deleting all superfluous vertices until there are no changes in successive runs. 

For example in \fig{wdia} ($iii$), $v_2$ is deleted from $\mathcal{P}$ because the path straight between $v_1$ and $v_3$ is shorter, and within $\Gamma$.

\item (ALTER) Given a triple of vertices $(v_i, v_{i+1}, v_{i+2}) \in \mathcal{P}$, if the candidate replacement path $\mathcal{P}_{ID}$ is shorter than the path $(v_i, v_{i+1}, v_{i+2})$ then replace $(v_i, v_{i+1}, v_{i+2})$ with $\mathcal{P}_{ID}$ (\fig{wdia}, ($v$)). The candidate replacement path, $\mathcal{P}_{ID}$, is calculated by running INIT with $p_1$ and $p_2$ replaced by $v_i$ and $v_{i+2}$.

For example in \fig{wdia} ($iv$), the path $(v_1, v_2, v_3)$ is longer than the path $\mathcal{P}_{ID}=(v_1, v^1_2, v_3)$ (green dashed line in ($iv$)) so the former is replaced with the latter in $\mathcal{P}$. The path created by INIT is marked as $\mathcal{P}_{I}$ in  ($iv$) in red.

\item (ITER) Iterate further DELETE and ALTER steps (in pairs) until there has been no change in $\mathcal{P}$ from one run to the next (i.e. convergence) (\fig{wdia}, ($vi$)).
\end{enumerate}

% diagram for finding the shortest path in W
\begin{sidewaysfigure}
% trim order l b r t
\psfrag{exp1}[]{$\mathcal{P}_1$}
\includegraphics[trim=0in 0.5in 0in 0.25in, width=9.5in]{mds/figs/wdia.pdf} \\
\caption{The green lines in ($i$) to ($vi$) show the steps forming the shortest path as the algorithm progresses from initial state to final, shortest path (bottom right). See \secref{mdsdist}.}
\label{wdia}
% generate /phd-smoothing/mds-writeup/figs/distanceexplanation.R
\end{sidewaysfigure}

Of course, if there is a direct path between $p_1$ and $p_2$ then the Euclidean distance between the points can be used and the above algorithm is not run.

Although it was not possible to theoretically prove that the algorithm will always converge to the shortest path, it is clear at least that the\label{cor-r34} algorithm will always converge (since this only requires that there be no change in the path for two consecutive iterations). Extensive simulations showed that the algorithm gave sensible results.

\section{Simulation experiments}
\label{mdssims}

In order to investigate the efficacy of \mdsap, a series of simulation experiments were performed. In all cases the results for \mdsap\ were compared to those of the current best method (the soap film smoother) and the standard approach that does not account for leakage (\tprs).

The \textsf{R} packages \texttt{mgcv} and \texttt{soap} were used for smoothing. Bespoke software was used to find the within-area distances and to perform Gower's interpolation (written by the author, see \secref{gds-software}). MDS projections were performed using the \texttt{cmdscale()} function in \textsf{R}. In all cases smoothing parameter estimation was performed using GCV (see \secref{GAMfitting})\label{cor-soft3}.

\subsection{The Ramsay horseshoe}

If a general method is to be useful it must first perform well on even a simple case such as the modified Ramsay horseshoe since it clearly illustrates the problem of leakage.

\subsubsection{Setup}

For the horseshoe, samples of 250 points were taken and normal noise added (for 3 different standard deviations: 0.1, 1 and 10). (These are the settings used in \cite{soap}.) Three methods were applied to the data. Predictions were then made over 718 points (including the sample locations). 200 realisations were generated and the effective degrees of freedom\label{cor-r35-1} (EDF) and MSE recorded for each replicate. The three methods used were as follows:

\begin{enumerate}
\item \emph{Thin plate spline}: bivariate \tprs\  with basis size 100.
\item \emph{Soap film smoother}: 32 knots evenly spread over a grid over the domain, cyclic spline on the boundary was of basis size 39.
\item \emph{\mdsap}: Used a \tprss\ of basis dimension 100. The initial MDS grid was made of points from $10\cross 20$ grid that lay inside the horseshoe.
\end{enumerate} 

Note that due to time and computational restrictions, the boundary was reduced from the 160 vertex polygon in the \texttt{fs.boundary()} function in \texttt{soap} to a 21 vertex polygon by only using every 8$^\text{th}$ vertex. This should not cause a major difference in results even if the soap film used the full boundary and \mdsap\ used only the reduced set of edges, since \label{cor-4s3}the objective is to allow the smoother to get a broad idea of the topology of the domain, rather than the minutiae of the boundary features. This is especially true given that the large number of boundary vertices are due to the attempt to approximate the curved parts of the shape, rather than other features such as peninsulae. In the simulations presented here, the soap film smoother and \mdsap\ used the same boundary.

\subsubsection{Results}

Predictions from a typical realisation can be seen from \fig{mds-ramsay-fit-1}, where the noise level was 1 with a sample size of 250. Both the soap film smoother and \mdsap\ are able to reproduce the main features of the true horseshoe function due to their ability to respect either the boundary (in the case of the soap film) or the geometry of the domain (in the case of \mdsap). The \tprs\ shows leakage as expected. When the noise level is high the \mdsap\ outperforms the soap film smoother in MSE terms (and is less variable).

%\begin{table}[t]
%\centering
%\begin{tabular}{c c c c}
% & & MSE & \\ 
%Noise level & \mdsap & Soap film & Thin plate\\ 
%\hline
%0.1  & 0.0032 (3$\cross10^{-5}$) & 0.0022 (3$\cross10^{-5}$) & 0.0402 (0.0008) \\ 
%1  & 0.0436 (0.0015) & 0.0482 (0.0014) & 0.2306 (0.0024) \\ 
%10  & 2.065 (0.121) & 3.070 (0.238) & 3.371 (0.113) \\ 
%\end{tabular}
%\begin{tabular}{c  c c c }
%&  & EDF & \\ 
%Noise level & \mdsap & Soap film & Thin plate\\ 
%\hline
%0.1 & 47.613 (0.35) & 39.164 (0.26) & 92.6 (0.102)\\ 
%1  & 8.383 (0.345) & 11.868 (0.401) & 46.607 (0.424)\\ 
%10 & 5.058 (0.332) & 5.586 (0.288) & 5.979 (0.251)\\ 
%\end{tabular}
%\caption{Mean MSE and estimated degrees of freedom (EDF) for the three models fitted to the modified Ramsay horseshoe function with standard errors (in brackets) over 200 realisations. Sample size was 250 with noise levels given above.}
%\label{ramsayresultstable}
%\end{table}

The EDFs in figure \ref{edf-mds-ramsay-boxplot} show that \mdsap\ fits a less complex model than the \tprs\ on average, and for the two higher error situations, has a lower EDF than the soap film. Given that this is coupled with a lower MSE, it appears that \mdsap\ simultaneously yields both a more accurate and less complex model than the soap film for the horseshoe when there is a high level of noise. When noise is lower, the soap film and \mdsap\ MSEs are still roughly of the same order. \Fig{mds-ramsay-boxplot} shows the logarithm of the per-realisation average MSE for each of the models at each error level.

% Ramsay fit with error=1 
\begin{figure}
\centering
\includegraphics[width=6in]{mds/figs/ramsay-fit-1.pdf} \\
\caption{From top left clockwise: truth for the (modified) Ramsay horseshoe (``truth''), \mdsap\ (``MDS''), the soap film smoother (``soap'') and thin plate regression splines (``tprs'') when 250 points sampled with noise level set to 1.\label{cor-4s4}}
\label{mds-ramsay-fit-1}
% generated (roughly) using ramsay-smooth-test.R
\end{figure}

\begin{figure}
\centering
\includegraphics[width=6in]{mds/figs/edf-mds-ramsay-boxplot.pdf} \\
\caption{Boxplots of the EDF per realisation of the Ramsay horseshoe for \mdsap\ (``mds''), the soap film smoother (``soap'') and \tprs\ (``tprs'') for noise levels 0.1, 1 and 10.}
\label{edf-mds-ramsay-boxplot}
% generated using phd-smoothing/mds/sim/boxplot-ramsay-edf.R
\end{figure}

% boxplot for Ramsay
\begin{figure}
\centering
\includegraphics[width=6in]{mds/figs/mds-ramsay-boxplot.pdf} \\
\caption{Boxplots of the logarithm of the MSE per realisation of the Ramsay horseshoe for \mdsap\ (``mds''), the soap film smoother (``soap'') and \tprs\ (``tprs'') for noise levels 0.1, 1 and 10 (left to right). As previously a paired Wilcoxon signed rank test showed that MSEs for \mdsap\ and \tprs\ were significantly different from the soap film smoother (at the 0.01 level) where the colours above indicate whether the MSE was better (green) or worse (red).}
\label{mds-ramsay-boxplot}
% generated using phd-smoothing/mds/sim/boxplot-ramsay.R
\end{figure}

Just as when the \sch\ transform was used to morph the domain, it is interesting to see what has happened to the distribution of points in space. \Fig{mdsrampoints} shows the effect of the transform on a regular grid of points (left) when they are projected into MDS space (right). The projection has also succeeded in parting the two arms of the horseshoe, reducing leakage (as can be seen in the realisations in \fig{mds-ramsay-fit-1}).

\begin{figure}
\centering
\includegraphics[width=6in]{mds/figs/mdsrampoints.pdf} \\
\caption{A regular grid over the Ramsay horseshoe (left) and its projection into MDS space (right).}
\label{mdsrampoints}
% generated using thesis/mds/figs/mdsrampoints
\end{figure}


\subsection{Peninsula domain}
\label{mds-wt2-sim}

\label{cor-r36}The Ramsay horseshoe is an easy domain to smooth over since it is clear that a transformation should be parting the two arms of the domain. In practice however, which parts of the domain should be separated the most may not be so clear cut. For this reason a more realistic, complex domain would provide a better insight into the efficacy of the method. The domain shown in \fig{wt2-truth} is a modification of the peninsula domain seen in \secref{sc-penin} with the three high density areas in the far left peninsula replaced with a single continuous gradient, making it a slightly easier domain. The domain was modified because it was found in the simulations in \secref{sc-penin} that it was difficult to verify visually that the smoother estimated the peaks in the peninsula correctly; a single gradient is easier to check by eye.

\subsubsection{Setup}

The simulations consisted of 200 realisations of 250 samples from the surface in \fig{wt2-truth}. Normal noise was added at three levels  0.35, 0.9, and 1.55 (corresponding to signal-to-noise ratios (SNRs) of 0.95, 0.75 and 0.5, respectively. SNRs were calculated as the mean squared correlation between true function value and the truth with error added). Mean squared error over 1253 prediction points (which included the sample points) was calculated and recorded, along with EDF for each model. The models fitted were:
\begin{enumerate}
\item \emph{Thin plate regression spline}: bivariate \tprs\ with maximum basis size 100. 
\item \emph{Soap film smoother}: cyclic spline on boundary of basis size 60, 109 internal knots evenly spaced on a grid over the domain.
\item \emph{\mdsap}: after transform a bivariate \tprs\ with maximum basis size 100. The initial MDS grid was $10 \cross 10$ (48 points were inside the domain).
\end{enumerate} 

% wt2 truth 
\begin{figure}
\centering
\includegraphics[width=3in]{mds/figs/wt2-truth.pdf} \\
\caption{True function for the domain with multiple peninsulae.}
\label{wt2-truth}
% generated (roughly) using wt2-smooth-test.R
\end{figure}

\subsubsection{Results}

% wt2 fit with error=0.9
\begin{figure}
\centering
\includegraphics[width=6in]{mds/figs/wt2-comp-09.pdf} \\
\caption{A typical realisation of fits from the multiple peninsulae domain when the noise level was set to 0.9 (SNR = 0.75) and the sample size was 250. The prediction grid was of size 1253. Clockwise from top left: the true function, prediction from: MDS projection smoothed with \tprs, the soap film smoother and \tprs.\label{cor-4s5}}
\label{wt2-comp-0.9}
% generated (roughly) using wt2-smooth-test.R
\end{figure}

Looking at a typical realisation in \fig{wt2-comp-0.9} (noise level = 0.9, SNR = 0.75, sample size 250), the \tprs\ shows signs of leakage across the two large peninsulae, whereas \mdsap\ and the soap film do not. The thin plate regression spline does, however, reproduce the peak in the lower right much more faithfully, the other two smoothing over it. In this realisation, \mdsap\ deals with the values inside the peninsula a little better than the soap film smoother (the contour lines are more similar to those in the true function). On the other hand, the soap film captures the shape of the lower right peak slightly more accurately.

\label{cor-4s6}Figures \ref{mds-wt2-boxplot} and \ref{mds-wt2-boxplot-edf} show boxplots of the MSE and EDF for the models above. The soap film smoother consistently has a statistically significantly lower MSE (at the 0.01 level). The soap film also tends to fit simpler models than the other two approaches except at the highest noise level.

% boxplot for wt2
\begin{figure}
\centering
\includegraphics[width=\textwidth]{mds/figs/mds-wt2-boxplot.pdf} \\
\caption{Boxplots of the logarithm of the MSE per realisation of the peninsula domain for the MDS approach (``mds+tp''), soap film smoother (``soap'') and \tprs\ (``tprs'') for noise levels 0.35, 0.9 and 1.55. A paired Wilcoxon signed rank test shows that the MSEs for the two other models were significantly different from the soap film smoother (and worse) at the 0.01 level.}
\label{mds-wt2-boxplot}
% generated using phd-smoothing/mds/sim/boxplot-wt2-1.R
\end{figure}

% EDF boxplot for wt2
\begin{figure}
\centering
\includegraphics[width=\textwidth]{mds/figs/mds-wt2-boxplot-edf.pdf} \\
\caption{Boxplots of the EDF for each model per realisation of the peninsula domain for the MDS approach(``mds+tp''), soap film smoother (``soap'') and \tprs\ (``tprs'') for noise levels 0.35, 0.9 and 1.55.}
\label{mds-wt2-boxplot-edf}
% generated using phd-smoothing/mds/sim/boxplot-wt2-1-edf.R
\end{figure}

As with the Ramsay horseshoe, it is interesting to see what the projection into MDS space has done to the distribution of the points in the domain. \Fig{wt2-2d-proj} shows points in the domain in Euclidean space and MDS space. There appears to be some high concentrations of points in the far left peninsula and in the right side in the MDS space. This is due to the projection of the points into 2-dimensional space, which can be easily seen in \fig{wt2-3d-proj} where the points have been projected into 3-dimensional space. The 3-dimensional projection also shows that there is separation between the smaller peninsulae in higher dimensions that cannot be seen in the 2-dimensional projection.

This high point density in the right side of the MDS space could be the reason for the poor reproduction of the function in that region seen in \fig{wt2-comp-0.9}. There appears to have been a severe\label{cor-r37} breakdown in isotropy in this part of the domain (and in the left peninsula), which the \tprs\ does not handle well. This must be accounted for in the smooth if accurate models are to be built.

% how the points are projected for wt2
\begin{figure}
\centering
\includegraphics[width=6in]{mds/figs/wt2-2d-proj.pdf} \\
\caption{A regular grid over the peninulae domain (left) and its projection into 2-dimensional MDS space (right).}
\label{wt2-2d-proj}
% generated using thesis/mds/figs/wt2-mds.R
\end{figure}

% how the points are projected for wt2 in 3D!
\begin{sidewaysfigure}
\includegraphics[width=9in]{mds/figs/wt2-3d-proj.pdf} \\
\caption{The peninsula domain projected into 3-dimensional MDS space. The plots show combinations of axes, note that the first panel is the same as the 2-dimensional projection shown in \fig{wt2-2d-proj}.\label{cor-r38}}
\label{wt2-3d-proj}
% generated using thesis/mds/figs/wt2-mds.R
\end{sidewaysfigure}

\subsection{Areas for improvement}

From this set of simulations areas for improvement to \mdsap\ can be seen. First, the above problem of the accuracy of the model (in terms of faithfully reproducing the function) needs to be addressed. Second, the calculation of the within-area distances by the algorithm given in \secref{mdsdist} has considerable computational cost, even in comparison to the soap film smoother basis setup (see also \secref{it-conc}). Table \ref{wt2time} shows the average timings for running \mdsap, \tprs\ and soap film smoothers over the peninsula domain.

\begin{table}[t]
\begin{centering}
\begin{tabular}{c c c c c c c}
 & \mdsap & Soap film & Thin plate spline\\ 
\hline
Fit & 84.852 & 24.678 & 0.402\\ 
Prediction &  155.400 & 29.395 & 0.125\\
\end{tabular}
\caption{Average time (in seconds) to fit and predict on a realisation of the peninsula domain for the three models considered above. Times are averaged over 100 realisations. For each realisation a sample of size 250 was taken, then a 1253 values were predicted.\label{cor-4s7}}
\label{wt2time}
\end{centering}
\end{table}

\label{cor-4s8} In order to be useful to practitioners \mdsap\ must perform at least as well as the soap film smoother in terms of accuracy (i.e. low MSE) and preferably take comparable computational time for model fitting. The next two sections address these issues.

\section{Making \mdsap\ faster}
\label{mds-faster}

\subsubsection{Calculating MDS by Lanczos iteration}

The \textsf{R} command used to perform the multidimensional scaling, \texttt{cmdscale}, uses the routine \texttt{eigen} in order to perform the requisite matrix eigen-decomposition. This routine will calculate a full eigen-decomposition of the matrix, even if only the first $k$ eigenvalues and/or eigenvectors are required. Using Lanczos iteration, only the first $k$ eigenvalues (in numeric or algebraic size order) will be calculated.

The  Lanczos procedure works by iteratively building a symmetric $i\cross i$ tridiagonal matrix (at the $i^{\text{th}}$ iteration) with eigenvalues which are approximately the same as the $i$ largest eigenvalues of the original matrix. Further detail is given in \citeb[pp. 335-337]{simonbook}.

The \texttt{igraph} library for \textsf{R} provides an interface to the C++ package \texttt{ARPACK++} which implements the Lanczos procedure. Replacing the \texttt{cmdscale} command with one that uses the \texttt{ARPACK++} interface provided by \texttt{igraph} will decrease the number of computations needed, thus making the calculation of the eigenvalues and vectors faster.

A quick benchmark shows that \texttt{ARPACK++} can compute the first two eigenvalues and vectors faster than just using \texttt{eigen} when the eigen-decomposition to be computed is of a large matrix. Generating a 1000 by 1000 symmetric matrix of normal random deviates with mean 0 and variance 1000, then performing an eigen-decomposition takes 1.68 seconds using \texttt{ARPACK++} and 3.26 seconds using \texttt{eigen} (averaged over 100 runs). This advantage drops once the matrix size is around 100 by 100 and the cost of calling the C++ code begins to dominate; in this case \texttt{ARPACK++} takes 0.037 seconds and \texttt{eigen} takes 0.034 (averaged over 100 runs). Given that the disadvantage is in the order of hundredths of a second and the advantage is a two-fold decrease in computational time, it makes sense to use the \texttt{ARPACK++} code in all cases.

% sim code is at ~/phd-smoothing/mds/lanczos/time-arpack.R

\subsubsection{Partial path calculation}

It is often the case that the points for which the within-area distances are required form a grid (the initial MDS grid setup, or when prediction points need to be found). This grid setup can be exploited since there are many sets of paths that are rather similar. These paths may perhaps only differ in their final vertex. When this is the case much computational time is wasted calculating similar paths, it would be useful to exploit this problem and use it to increase the speed of the path calculation.

By appending the points between which the within-area distance is required to either end of one of a series of pre-calculated base paths, then optimizing this new path using the DELETE and ALTER steps as before, it is hoped that the computational time will be reduced. Using base paths will hopefully removes the expensive calculation in the middle of the path, where perhaps the bulk of the interactions with the boundary take place.

The algorithm is as follows, with notation and routines (INIT, DELETE, ALTER and ITER) identical to those in \secref{mdsdist}:
\begin{enumerate}
 \item Begin by creating a sparse grid of within the simple polygon $\Gamma$ and calculate the ($M$, say) non-Euclidean within-area paths between all pairs of points in the grid, as in \secref{mdsdist}. Store these paths as $\mathcal{P}_1,\ldots, \mathcal{P}_M$.
\item For each unique pairing of $p_i$ and $p_j$ in the full data set, calculate the path using one of the following:
	\begin{enumerate}
	\item Find a $\mathcal{P}_k$ such that the path between $p_i$ and one end of $\mathcal{P}_k$ and $p_j$ and the other end of $\mathcal{P}_k$ is Euclidean within $\Gamma$. Join $p_i$ and $p_j$ onto the appropriate ends of $\mathcal{P}_k$ and alternate between DELETE and ALTER steps until convergence.
	\item If no $\mathcal{P}_k$ can be found calculate the path between $p_i$ and $p_j$ as in \secref{mdsdist}. 
	\end{enumerate}
\end{enumerate}

Note that those paths between points in the sparse grid which are Euclidean are not stored since it is always at least as expensive to store, add and optimise those paths then calculating them from scratch. If the required path is Euclidean anyway, then retrieving a Euclidean path, adding in $p_i$ and $p_j$, and then iterating over ALTER and DELETE steps to make it both the shortest and a Euclidean path will take longer than just creating a Euclidean path to begin with. If the path between $p_i$ and $p_j$ is non-Euclidean then the non-Euclidean part of the path must lie outside $\mathcal{P}_k$ (by definition, if $\mathcal{P}_k$ were Euclidean) and therefore will take the same number of operations to find the boundary crossing points and calculate the shortest path around the feature locally as it will to calculating the whole path from scratch.


\subsubsection{Simulation - Lanczos and partial path calculation improvements}

\begin{table}[t]
\begin{centering}
\begin{tabular}{c c c c c c c}
%  no speedup           speedup
 & \mdsap & \mdsap (\textit{pp}) & Soap film & Thin plate\\ 
\hline
Fit & 84.852 & 18.653 & 24.678 & 0.402\\ 
Prediction & 155.400 & 53.511 & 29.395 & 0.125\\
\end{tabular}
\caption{Average time (in seconds) to fit and predict on a realisation of the peninsula domain for the three models considered above. Times are averaged over 100 realisations using \textsf{R}'s built-in \texttt{system.time} function. For each realisation a sample of size 250 was taken, then a 1253 values were predicted. For the \mdsap\ columns \textit{pp} indicates the cases where the partial paths were pre-calculated with the Lanczos procedure used to find the eigen-decomposition of the distance matrix, those not marked use the algorithm given in \secref{mdsdist} and did not use the Lanczos procedure.}
\label{wt2itimetable}
\end{centering}
\end{table}

Taking both the Lanczos procedure and the partial path calculation together, a simulation was run to find the improvements in terms of computational time for the double peninsulae domain. Average time for both model fitting and prediction are given in table \ref{wt2itimetable} for 100 realisations. 

The differences between the first two columns are striking. The partial path calculation has dramatically reduced the computational time for the calculation of the entries of the distance matrix, making it faster than the soap film smoother for the model fitting, and reducing the prediction time to a third of its previous value. The soap film smoother's prediction is relatively low since the bulk of the computational time is spend on solving the PDEs necessary to find the basis functions (see \secref{SF}). The \tprs\ times are shown to give a comparison for the time actually taken to fit the model, the remaining time for \mdsap\ is taken up by calculating the distances and performing the MDS.

Now that the computational time has been considerably reduced, the next task is to improve \mdsap's fit to the data.

\section{Using penalty adjustments to correct for\\ squashing}
\label{mds-penadjust}

Figures \ref{wt2-2d-proj} and \ref{wt2-3d-proj} show that the points in the peninsulae domain have been squashed together in some areas. In particular, this occurs in the peninsulae themselves. Since the motivation here is to improve estimates in the peninsulae, it would be extremely unfortunate if the transformation was detrimental to the smoother's performance in these areas (albeit in a different way). The squashing is similar to that seen in \secref{sch-crowding} when the \sch\ transform was used to transform the domain, although not as severe as crowding seen, the squashing together of points can still be problematic (as was seen in \secref{sc-penin}). 

Uneven point density can cause problems for the smoother since the measure of smoothness will change.  Squashing space together may make the data appear more variable than it in fact is (see figure \ref{1dadjust} for a 1-dimensional example). Moving into two dimensions, an isotropic smooth seems like an unrealistic choice if the point densities are different in each direction. One might expect that the amount of smoothing required would be a function of the density of points at that location.

The higher MSEs shown in figure \ref{mds-wt2-boxplot} might be explained by the change in density of points in the domain after it has been transformed into MDS space. In this case adjusting the thin plate spline penalty in order to take into account the change in point density in MDS space might improve performance.

An alternative approach would be to use a tensor product basis (of, for example, P-splines as was used in \secref{sc-sims}) where there is a separate smoothing parameter for each dimension. However, thin plate regression splines have other appealing properties (in particular the low-rank approximation avoids knot placement issues, see \secref{GAMtprs}). A tensor product smooth may also still run into problems in each dimension with regard to the squashing of space. For these reasons, this section explores the utility of penalty adjustments based on point density for thin plate regression splines.

\subsection{Overview}

\citeb{wood2000} shows that if one of the covariates of a thin plate spline ($x_2$ say) is transformed such that $x_{2}^\prime=x_{2}/k$, then $f(x_1,x_2^\prime k)$ will give the same fit as $f(x_1,x_2)$ (ie. the fit will be the same under the new coordinates), if the penalty is changed to:
\begin{equation}
\int\int_{\mathbb{R}^2} \left ( \frac{\partial^2 f}{\partial x_1^2} \right )^2 + 2k\left ( \frac{\partial^2 f}{\partial x_1 \partial x_2} \right )^2 + k^3\left ( \frac{\partial^2 f}{\partial x_2^2} \right )^2 \text{d}x_1 \text{d}x_2,
\label{adjustedintegral}
\end{equation}
from the usual \tprs\ penalty (see \secref{GAMtprspenalty}).

This approach will only handle a linear rescaling in one dimension; in the case of the MDS distortions, non-linear re-scalings in two dimensions must be addressed. To generalise \eqn{adjustedintegral} to the non-linear two-dimensional case a function of the coordinates must be found which gives the change in density for each point in the domain. Denote such a function $\mathcal{L}^*(x_1,x_2)$. 

Including the function in the integral should allow the penalty to be adapted according to the degree to which space has been squashed, thus getting around the spatial inhomogeneity which appears to be affecting the model. The calculation of $\mathcal{L}^*(x_1,x_2)$ is elaborated on below.

Given that the function $\mathcal{L}^*(x_1,x_2)$ is known, the penalty is given as:
\begin{equation}
\int\int_{\Gamma^\prime} \mathcal{L}^*(x_1,x_2) \left \{ \left (\frac{\partial^2 f(x_1,x_2)}{\partial x_1^2}\right )^2 + 2\left (\frac{\partial^2 f(x_1,x_2)}{\partial x_1 \partial x_2}\right )^2 + \left (\frac{\partial^2 f(x_1,x_2)}{\partial x_2^2}\right )^2\right \} \text{d}x_1 \text{d}x_2.
\label{kdeadjust}
\end{equation}
Note the change of integration domain from $\mathbb{R}^2$ to $\Gamma^\prime$, the transformed domain. This is because $\mathcal{L}^*(x_1,x_2)$ is not defined outside of $\Gamma^\prime$.

Before moving straight to the 2-dimensional case, the method was tested in one dimension.

\subsection{Penalty adjustments in one dimension}

Before implementing this approach in full, a 1-dimensional test was run. The function:
\begin{equation}
g(x)=0.2x^{11}\left \{ 10(1-x) \right \}^6+10(10x)^3(1-x)^{10},
\label{hardfcn}
\end{equation}
over the range [0,1] was used. The function was then split into four sections ($[0,0.4]$, $(0.4,0.6]$, $(0.6,0.8]$ and $(0.8,1]$) and in each of these sections $x$ values were multiplied by 20,1,0.05 and 1, respectively (effectively contracting or expanding space)\label{cor-r39}. The function and its squashed form are shown in the top left and right panels (respectively) of \fig{1dadjust}. The function was evaluated at 100, equally spaced, points over the interval $[0,1]$. These points were then used as the sample (with no noise added), a \tprs\ was fitted, and predictions made at same points yielding the blue lines in the lower two plots. The left plot shows the predictions in the transformed space and the right in the original space. The green line was produced using the same procedure but with a \tprs\ with the adjusted penalty. As can be seen from the plot, the fit has been improved greatly, but how is the adjustment calculated?

% 1d adjustment 2x2 diagram
\begin{figure}
\centering
\includegraphics[width=6in]{mds/figs/1dadjust.pdf} \\
\caption{Using penalty adjustments to fit a regression spline to \eqn{hardfcn} after it has been squashed. The function in the top left is squashed to the form in the top right. The bottom left plot shows the fit from a \tprs\ (blue) and a \tprs\ with adjusted penalty (green) in the transformed space. The bottom right shows the same fit in the untransformed space. Clearly, the penalty adjustment improves the fit.}
\label{1dadjust}
% generated by thesis/mds/figs/tpintexp.R
\end{figure}

\subsubsection{Penalty adjustment calculation}

For the moment let us take $f$ to be a 1-dimensional smooth function. The formula for the $ij^\text{th}$ element of the penalty matrix given in (\ref{pen-quadform}) can be adapted in the following way: 
\begin{equation*}
\mathbf{S}_{ij}= \int_a^b \mathcal{L}^*(x) \frac{\partial^2 b_i(x)}{\partial x^2}\frac{\partial^2 b_j(x)}{\partial x^2} \text{d}x = \int_a^b \mathcal{L}^*(x) b^{\prime\prime}_i(x) b^{\prime\prime}_j(x) \text{d}x,
\end{equation*}
where $b_j$ is the $j^\text{th}$ basis function of $f$ and letting a prime indicate differentiation with respect to $x$. The integral can then be approximated by the midpoint rule as:
\begin{equation}
\mathbf{S}_{ij}= \frac{b-a}{K}\sum_{k=1}^K \mathcal{L}^*(x_k) b^{\prime\prime}_i(x_k) b^{\prime\prime}_j(x_k) \quad \text{for} \quad x_k=a+\frac{(k-0.5)(b-a)}{K},
\label{midpointS}
\end{equation}
for $k=1,\dots, K$. Second derivatives are evaluated by finite differences in the usual manner:
\begin{equation}
\label{bfinitediff}
b^{\prime\prime}_i(x) = \frac{ b_i(x+2\epsilon) - 2b_i(x+\epsilon) + b_i(x)}{\epsilon^2}.
\end{equation}
For the sake of efficiency, a $K\cross J$ matrix, $\mathbf{D}$, is calculated with $kj^\text{th}$ element:
\begin{equation}
\mathbf{D}_{kj}=\sqrt{\mathcal{L}^*(x_k)} b^{\prime\prime}_j(x_k),
\label{oneDD}
\end{equation}
for $x_k$ as above. Then $\mathbf{S}$ may be calculated as:
\begin{equation*}
\mathbf{S}=\frac{b-a}{K}\tr{\mathbf{D}}\mathbf{D}.
\end{equation*}
Note that in previous chapters, the penalty was computed analytically, rather than numerically  in \texttt{mgcv} so these calculations were unnecessary. However, since a different penalty is evaluated here, it must be calculated numerically.\label{cor-r40}

In this example $\mathcal{L}^*(x)$ was simply calculated using the inverse of the cube of the (known) factor by which the relevant part of the domain (given above) was squashed. 

\subsubsection{Checking that the adjustment works}

\Fig{1dadjust} shows that the adjustment faithfully reproduces $g(x)$ for the zero error case, fitting a much more sensible model than the standard \tprs. To check that this is true more generally, the smoothing parameter ($\lambda$) was specified (rather than being automatically selected) so that the models with modified and unmodified penalties would have the same EDF. \Fig{1dedfdia} shows such an experiment. Using \eqn{hardfcn} and adding standard normal noise (multiplied by 0.4), the smoothing parameter was set so that the EDF would be 71, 19 and 42 (working down the diagram). The plots show that the adjustment deviates from truth at most as badly as the vanilla \tprs\ but overall corrects some of the departures from the truth, even in presence of error when the model flexibility is restricted.

% 1d adjustment EDF comparison diagram
\begin{figure}
\centering
\includegraphics[width=5.5in]{mds/figs/1dedfdia.pdf} \\
\caption{Predictions in transformed and untransformed (left and right columns respectively) for \tprs\ (blue line) and penalty adjusted \tprs\ (green line) fits to the function in \eqn{hardfcn} when the smoothing parameter was pre set to give EDF of 71, 19, and 42 (top to bottom).}
\label{1dedfdia}
% generated by thesis/mds/figs/tpintexp.R
\end{figure}


\subsection{Penalty adjustments in two dimensions}
\label{cor-r41}

Using a similar procedures as for one dimension, the two dimensional case can be addressed. Again looking at the $ij^\text{th}$ element of $\mathbf{S}$:
\begin{align*}
\mathbf{S}_{ij}= \int\int_{\Gamma^\prime} \mathcal{L}^*(x_1,x_2) \Bigg ( & \frac{\partial^2 b_i(x_1,x_2)}{\partial x_1^2}\frac{\partial^2 b_j(x_1,x_2)}{\partial x_1^2}+2\frac{\partial^2 b_i(x_1,x_2)}{\partial x_1 \partial x_2}\frac{\partial^2 b_j(x_1,x_2)}{\partial x_1 \partial x_2}+ \\ 
& \frac{\partial^2 b_i(x_1,x_2)}{\partial x_2^2}\frac{\partial^2 b_j(x_1,x_2)}{\partial x_2^2} \Bigg )  \text{d}x_1\text{d}x_2.
\end{align*}
Matrices analogous to \eqn{oneDD} can be constructed using the finite differences from \eqn{bfinitediff} for differentials $x_1$ and $x_2$ individually and
\begin{equation*}
\frac{\partial^2 b_i(x_1,x_2)}{\partial x_1 \partial x_2} = \frac{ b_i(x_1+\epsilon,x_2+\epsilon) - b_i(x_1+\epsilon,x_2) - b_i(x_1,x_2+\epsilon) + b_i(x_1,x_2)}{\epsilon^2},
\end{equation*}
for the cross term. The matrices then take the form:
\begin{equation*}
[\mathbf{D}_{x_1}]_{kj}=\sqrt{\mathcal{L}^*(x_{1k},x_{2k})} \frac{\partial^2 b_j(x_{1k},x_{2k})}{\partial x_1^2},
\end{equation*}
\begin{equation*}
[\mathbf{D}_{x_2}]_{kj}=\sqrt{\mathcal{L}^*(x_{1k},x_{2k})} \frac{\partial^2 b_j(x_{1k},x_{2k})}{\partial x_2^2},
\end{equation*}
\begin{equation*}
[\mathbf{D}_{x_1 x_2}]_{kj}=\sqrt{\mathcal{L}^*(x_{1k},x_{2k})} \frac{\partial^2 b_j(x_{1k},x_{2k})}{\partial x_1 \partial x_2}.
\end{equation*}
So $\mathbf{S}$ may then be written as:
\begin{equation*}
\mathbf{S}=\tr{\mathbf{D}_{x_1}}\mathbf{D}_{x_1} + \tr{\mathbf{D}_{x_1 x_2}}\mathbf{D}_{x_1 x_2} + \tr{\mathbf{D}_{x_2}}\mathbf{D}_{x_2}.
\end{equation*}
Where the partial derivative evaluation points ($x_{1k}$ and $x_{2k}$) now form a grid for the integration to be calculated. First defining $x_{1k}$ and $x_{2k}$ analogously to (\ref{midpointS}):
\begin{equation*}
x_{1k}=a_{x_1}+\frac{(k-0.5)(b_{x_1}-a_{x_1})}{K},\quad
x_{2k}=a_{x_2}+\frac{(k-0.5)(b_{x_2}-a_{x_2})}{K},
\end{equation*}
for $k=1,\dots,K$. The integration grid may then be constructed as points in the $x_1$ direction:
\begin{equation*}
\left \{x_{11},x_{11},x_{11},\dots, x_{12}, x_{12}, x_{12},\dots, x_{1K}, x_{1K}, x_{1K}\right \},
\end{equation*}
and $x_2$ direction:
\begin{equation*}
\left \{x_{21},x_{22}, x_{23},\dots, x_{2K},x_{21},x_{22}, x_{23},\dots, x_{2K},\dots\right \}.
\end{equation*}
Finally, those $(x_{1k},x_{2k})$ that do not lie inside the boundary in MDS space are removed leaving only those points that lie inside (so that the integration is performed over $\Gamma^\prime$).

\subsubsection{Finding $\mathcal{L}^*$}
%Would the Jacobian be very slow? Need a dense grid, so we would have f(x...) and there would be an x for each data point?
\label{mds-findinglstar}

Up to this point it has been assumed that the factors by which space has been stretched are known. In practice, this is not the case and they dependent on the transformation. A simple, heuristic way to calculate $\mathcal{L}^*(x_1,x_2)$ is to think of the degree to which space has been stretched or squashed as a change in the density of the points in space. By estimating the density of the points over a grid in MDS space (by simply counting the number of points from a grid in the data space are mapped to a particular square in a grid in the MDS space), an approximation to the density change over the whole domain can be found.

The general idea is to make $\mathcal{L}^*(x_1,x_2)$ a function of the change in density of points caused by projecting the original space into MDS space. Assuming that the density in the untransformed space is $1$ everywhere, it is only necessary to calculate the density in MDS space. Having calculated the point density in MDS space, $\mathcal{L}^*(x_1,x_2)$ is just some function of this density. 

Although this method of approximating the density change is somewhat ad hoc, it should be able to highlight the large-scale changes in density which are causing the most problems when smoothing in MDS space. The point maps in figure \ref{wt2-2d-proj} show that the density change in density is relatively smooth and does not have any sudden jumps, so a relatively coarse approximation to the density should suffice.

In order to find calculate the point density in MDS space, the following steps are performed:
\begin{enumerate}
\item A grid in the original space is mapped into the MDS space. Given how computationally demanding using a dense grid would be, a sparse grid was used and then interpolated. This consisted of taking 10 equally spaced points on each side of the square in the sparse grid and drawing lines between points on opposing sides. Extra points were then added where the lines crossed (along with those points lying on the boundary of the square itself).
\item The interpolated points were then used to estimate the overall point density in MDS space by simply counting the number of points there were in each of a set of squares made from the integration grid. The count per cell is a function of location and is denoted $\mathcal{L}(x_1,x_2)$. An example is shown in the bottom right plot of figure \ref{densgrid} for the double peninsulae domain. 
\item Then define $\mathcal{L}^*(x,y)$ as the function
\begin{equation*}
\mathcal{L}^*(x_1,x_2)=\frac{1}{ \left \{\mathcal{L}(x_1,x_2) +1 \right \}^{3/2}}.
\end{equation*}
\label{cor-4s9}Adding one to the denominator avoids division by zero when evaluating $\mathcal{L}^*(x_1,x_2)$. The fact that $\mathcal{L}^*(x_1,x_2)$ is a piecewise function should not be too worrying since the aim here is to address the broader problems with the change in spatial density, not the fine-gained details.
\end{enumerate}

Note that the power is now $\frac{3}{2}$. This is since the contraction/expansion in each direction individually is not known, but rather the overall change. As such $\frac{3}{2}$ is used rather than the cubic on $x$ and $y$ and unitary on the cross term. Several other options were also tested (including changing the power and removing $+1$ in the denominator) however it was found that the above formulation provided the best results and so only those results are shown here.

\begin{figure}
\centering
\includegraphics{mds/figs/densgrid.pdf} \\
\caption{The grids used to calculate $\mathcal{L}^*(x_1,x_2)$ for the double peninsulae domain. The red grid in the top left figure is mapped to the red grid in the top right panel. The red points in the top right are then used as the basis for the interpolation in the bottom left. The number of green points in each of the squares made from the black points in the bottom left plot are used to calculate the spatial density in that square. The heat map in the bottom right shows the values of $\mathcal{L}(x_1,x_2)$ (i.e. the density of the green points), here red is low density, yellow is high. Note that some of the points lie outside of the boundary in the MDS projections. This is due to the boundary in MDS space being the straight line interpolant of the vertices of the boundary in the original space.}
\label{densgrid}
% generated by phd-smoothing/mds/wt2-intexp.R and intexp/smooth2.c.R with comments removed
\end{figure}

\subsubsection{Checking that the adjustment works}

In order to make sure that the adjustment works in both the known and unknown contraction/expansion case, a small simulation was run. In this case a surface consisting of two bivariate normal distributions (mean vectors $(0,-0.5)$ and $(0,0.5)$, covariance matrix diagonal entries $(0.2,0.1)$) were sampled from (sample size 300) and then noise added from a $\text{normal}(0,0.05)$ distribution. The surface was then divided into its four constituent quadrants about the origin and squashed according to the following factors (in $(x_1,x_2)$ pairs, in order top left, top right, bottom left, bottom right): ((0.3,5), (1,5), (0.3,1), (1,1)). \label{cor-r39-2}This is equivalent to what happened in the 1-dimensional case above, but per-dimension (so for $x_1$, the values are replaced by $x_1/0.3$ and for $x_2$ by $x_2/5$ and so on).

The samples were then used to fit a standard \tprs\ model, \tprs\ with adjusted penalty (with the factors above used as the values of $\mathcal{L}^*(x_1,x_2)$) and a \tprs\ with $\mathcal{L}^*(x_1,x_2)$ estimated from the density of the grid once it was transformed into MDS space\label{cor-r42}. The mean squared error between the truth and prediction over a dense (50 by 50) grid was then calculated.

\label{cor-4s10}The simulation results show that there is a decrease in MSE when the expansion/contraction of the space is taken into account (MSEs were: 2.355, 2.30 and 2.305 for thin plate regression splines, known stretch and estimated stretch, respectively). Unfortunately this didn't offer the same visual improvement as the 1-dimensional case.

The next section puts the adjusted penalty approach to the test on the peninsulae domain seen previously and the Aral sea data set discussed in \secref{intro-GAM}.

\subsection{Wider simulations and real data}

\subsubsection{Peninsulae domain}
\label{wt2bigsim}

Using the same setup as in \secref{mds-wt2-sim}, for each error level (0.35, 0.9, and 1.55), 200 realisations were generated. From these 250 samples were drawn to fit the model, predictions were made over a grid of 1253 points with MSE and EDF recorded per model for each simulation. 

The models that were fitted were:
\begin{enumerate}
\item \emph{tprs}: \tprs\ with basis size 140.
\item \emph{mds+tp}: \mdsap\ using a \tprs\ with basis size 140.
\item \emph{mds 3D}: \mdsap\ using a 3-dimensional \tprs\ with basis size 140. Here MDS was used to project the data into three dimensions rather than two.
\item \emph{mds+adj}: \mdsap\ using a \tprs\ with basis size 140, with penalty adjustments.
\item \emph{soap}: soap film smoother using 109 internal knots evenly spaced on a grid over the domain, with boundary basis size 60.
\end{enumerate}

The results from mds+adj are actually worse than those from just mds+tp in all but the lowest noise case. \Fig{big-wt2-mses} shows boxplots of these results.

A Wilcoxon signed rank test, matching pairs between realisations showed that there was a significant difference between the MSE of each model and the soap film smoother. As can be seen from \fig{big-wt2-mses}, soap outperforms all of the other methods on this domain, with MDS 3D coming in second. 

Adding just one more dimension improves the MSE more than using the complicated adjustment terms described above. Perhaps such an approach deserves further attention. The plots in \fig{wt2-3d-proj} show that projecting into an additional dimension allows for further separation of both a large and small peninsulae, which should avoid leakage as well as perhaps help with the anisotropy (since in the extra dimension the far left peninsula has a greater width). 

As would be expected, the method using the adjusted penalty had a lower EDF than the other methods aside from the vanilla \tprss\ (see figure \ref{big-wt2-edfs}), since it is penalizing more heavily. Interestingly the model using the 3-dimensional projection also has a low EDF, showing that there may be some utility in using such a method especially considering its relatively low MSE.

It seems that the penalty adjustments have not been useful for this domain. It is especially interesting to see that the addition of one dimension provides much better models than a penalty-based approach.

Finally, note that there were three realisations omitted (for all models) in figures \ref{big-wt2-mses} and \ref{big-wt2-edfs}. In these realisations the soap film smoother failed to fit the model due to knot placement. These can be safely removed as, in practice, the computer would inform the user that the knot placement was not appropriate and the knot layout could be altered.

% big wt2 sim MSEs
\begin{figure}
\centering
\includegraphics[width=\textwidth]{mds/figs/big-mds-wt2-boxplot.pdf} \\
\caption{Logarithm of per realisation average mean squared error for the double peninsulae domain. Models are in groups of five for each error level (0.35,0.9,1.55). In all cases, a Wilcoxon signed rank test showed that MSEs for all models were significantly different from the soap film smoother (at the 0.01 level).}
\label{big-wt2-mses}
% generate /phd-smoothing/mds/sim/boxplot-wt2.R
\end{figure}

% big wt2 sim MSEs
\begin{figure}
\centering
\includegraphics[width=\textwidth]{mds/figs/big-mds-wt2-boxplot-edf.pdf} \\
\caption{Per realisation EDFs for the double peninsulae domain. Models are in groups of five for each error level (0.35,0.9,1.55).}
\label{big-wt2-edfs}
% generate /phd-smoothing/mds/sim/boxplot-wt2-edf.R
\end{figure}

\subsubsection{Aral sea}
\label{aral-sec}

The Aral sea is located between Kazakhstan and Uzbekistan. It has been steadily shrinking since the Soviet government diverted the sea's two tributaries in order to irrigate the surrounding desert during the 1960s. The NASA SeaWifs satellite collected data on chlorophyll levels in the Aral sea (see also \secref{intro-GAM} \label{cor-r43}and \cite{soap}) over a series of 8 day observation periods from 1998 to 2002. The 496 data are averages of the $38^\text{th}$ observation period. Smooths were fitted to the spatial coordinates (Northings and Eastings) with the logarithm of chlorophyll concentration as the response\label{cor-r44} (and assuming that the response was Gamma distributed as in \cite{soap}).

A \tprs, \mdsap\ and soap film were all fitted to the data. In summary the setup for each model was:
\begin{enumerate}
\item \emph{tprs}: \tprss with basis size 70.
\item \emph{soap}: the soap film smoother using a 12 by 12 grid of knots (74 were inside) and a boundary smooth with basis size 49.
\item \emph{mds}: \mdsap\ using thin plate regression splines with basis size 70.
\end{enumerate}

\begin{figure}
\centering
\includegraphics{mds/figs/aral-fit.pdf} \\
\caption{Raw data and predictions from the models fitted to the Aral sea chlorophyll data. Clockwise from top left: raw data, \tprs, soap film smoother, and \mdsap.}
\label{aral-fit}
% generated by phd-smoothing/mds/wt2-intexp.R and intexp/smooth2.c.R with comments removed
\end{figure}

The models were then used to predict over a grid of 496 points to create the heat maps shown in \fig{aral-fit}. The fits are broadly similar, with the \tprs\ showing some signs of leakage around (-50,-50). Both \mdsap\ and the soap film smoother do not have this problem. The contour lines for all of the models look roughly the same in the main part of the sea, but in the smaller lobe, \mdsap\ is rather different from both the soap film smoother and \tprs. 

Although the leakage is avoided, there appear to be some strange artefacts in the smooth. Ovals of higher chlorophyll appear in the smaller lobe when the \mdsap\ is fit to the data, along with contours close to the far left of the smaller lobe. Looking at a point plot in MDS space (\fig{aral-pp}) reveals why this might be happening. As can be seen from the figure, the smaller lobe has been severely squashed which will clearly have an adverse effect on the smoother.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{mds/figs/aral-pp.pdf} \\
\caption{The prediction points for the Aral sea data set (left), with their projection into MDS space (right).}
\label{aral-pp}
% generated by phd-smoothing/mds/aral/pointplot.R
\end{figure}

The \mdsap\ with adjusted penalty was also used to fit the model, using the same basis as above. The predicted surface given by the model is shown in \fig{aral-adj-fit}. Again, the same artefacts are clearly visible in the smaller lobe of the region.

\begin{figure}[t]
\centering
\includegraphics[width=3in]{mds/figs/aral-adjfit.pdf} \\
\caption{Predictions for the Aral sea using \mdsap\ with adjusted penalty.}
\label{aral-adj-fit}
% generated by ???
\end{figure}

As in the peninsula case above, a 3-dimensional MDS projection was also used and a \tprs\ fitted. The artefacts are less prominent and the surface looks much more like the one given by the soap film smoother. Again, the 3-dimensional projection shows much promise especially given the minimal extra cost to running the additional model (if the within-area distances are already calculated, only the MDS projection needs to be calculated, and the \tprs\ fitted).

\begin{figure}
\centering
\includegraphics[width=3in]{mds/figs/aral-3d.pdf} \\
\caption{Predictions for the Aral sea using a 3-dimensional projection into MDS space with \mdsap.}
\label{aral-fit-3d}
% generated by ???
\end{figure}

\section{Problems with the methodology so far}
\label{mds-problems}

At this point two outstanding issues must be addressed. The first is that, following the experiments above, there are issues with artefacts in the smooths produced by \mdsap\ when a low dimensional projection is used. The effect of these artefacts is decreased when the projection is taken into higher dimensions, however, the artefacts are still present when a 3-dimensional projection is used and high dimensional smoothing can be rather tricky. 

What follows is an explanation of why the artefacts occur, and how high dimensional smoothing can help. This section explores these two problems, explains what is going wrong and sets out what is needed for a solution.

\subsection{Why adjusting the penalty is not the solution}
\label{pensuck}

The simulations above show that the adjusted penalty scheme does not offer any advantage over using \mdsap\ with the standard penalty. Before running the simulations, several different functions of the MDS point density ($\mathcal{L}^*$) were compared. All resulted in worse smooths (in MSE terms) than the function that was finally settled on. Investigating the good performance of the 3-D projection model goes some way to explaining why the penalty adjustment doesn't offer much improvement.

Looking at the plots of the prediction points in MDS space for the peninsulae domain (\fig{wt2-2d-proj} and \fig{wt2-3d-proj}) it is easy to see that in two dimensions, the first peninsula has been squashed to a line. 

By truncating $\tilde{\mathbf{X}}^*$ in the MDS procedure (\secref{MDStechdet}) the information in $\mathbf{D}$ relating to the width of the peninsula has been lost. Projecting into higher dimensions reduces the truncation (smaller eigenvalues of $\mathbf{D}$ and their corresponding eigenvectors are use to construct $\tilde{\mathbf{X}}^*$) and therefore more information about the relative positions of the points is included (this can be seen for the peninsula domain in \fig{wt2-3d-proj}). The adjusted penalty attempts to account for this squashing by allowing a more flexible model to be fit in areas where the point density is higher. However, what is not taken into account is that some of the points in the peninsula are projected on top of (or on the wrong side of) one another. In other words, the 2-dimensional MDS projection makes the points lose their ordering. 

As a simple, unidimensional example, take three points, $a, b, \text{ and } c$ in the top line of \fig{linedia}. The projection could squash them in the way shown on the second line and then the penalty adjustments as described in \citeb{wood2000} could be used to correct the squashing. However, with MDS the situation shown in the bottom line of \fig{linedia} can occur (changing the order of $a, b, \text{and}\ c$). This phenomena is mentioned in \citeb[pp. 572-573]{elements}.

In the MDS projection we can see this happening for the peninsulae domain in figure \ref{wt2-3d-proj}. The projection takes a side-on view of the peninsula, making the points lose their ordering. In this case, the penalty adjustment can't save the model. If the ordering of the points is not guaranteed, then the smoother's job is potentially impossible, especially moving into two dimensions.

\begin{figure}
\centering
\includegraphics[width=3in]{mds/figs/linedia.pdf} \\
\caption{An illustration of how spatial mappings can squash points (middle line) and reorder them (bottom line) from their original configuration (top line).}
\label{linedia}
% generated by thesis/mds/figs/linedia.R
\end{figure}

\subsection{Why moving to higher dimensions is tricky}
\label{nohigherdim}

In the two cases above, moving into three dimensions allowed \mdsap\ to more accurately reproduce the true function. The extra dimension allows for more information to be included in the projection giving ``width'' to parts of the domain that appear extremely thin in the 2-D projection.

It seems then that there is some milage in taking a 3-dimensional projection to solve this problem without the need for penalty adjustments. However, unfortunately, this is not the case in general. \Fig{mds-comb} shows a long domain with three peninsulae at each end. The MDS projection of the domain in two dimensions gives the points in \fig{mds-comb-2d}. There are only four peninsulae in this figure, not the six which were in the original. Using the colours in \fig{mds-comb} and \fig{mds-comb-2d}, one can see the separation of the larger peninsulae and that the smaller peninsulae have been positioned end-to-end. \label{cor-r45}There could be interesting features in the response in these smaller peninsulae and thus leakage would be undesirable. This behaviour may well be worse than simply incurring leakage by using a standard smoother.

\begin{figure}
\centering
\includegraphics[width=3in]{mds/figs/comb.pdf} \\
\caption{The ``comb'' domain from \secref{nohigherdim}.}
\label{mds-comb}
% generated by thesis/mds/figs/comb.R
\end{figure}

\begin{figure}
\centering
\includegraphics[width=3in]{mds/figs/comb-2d.pdf} \\
\caption{Two-dimensional MDS projection of the domain in \fig{mds-comb}, note that there are only four ``legs'' here not the six that should be there, as in \fig{mds-comb}.}
\label{mds-comb-2d}
% generated by thesis/mds/figs/comb.R
\end{figure}

Adding an extra dimension could help. Taking a 3-dimensional projection, \fig{mds-comb-3d} is produced; however there is still no separation of the smaller peninsulae. Moving into four dimensions (\fig{mds-comb-4d}) we begin to see separation in the smaller peninsulae. However, even in four dimensions the separation is not particularly large and leakage could still occur.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{mds/figs/comb-3d.pdf} \\
\caption{The MDS projection of the domain in \fig{mds-comb} into three dimensions. Note that there is still no separation in for the smaller peninsulae.}
\label{mds-comb-3d}
% generated by thesis/mds/figs/comb.R
\end{figure}

\begin{figure}
\centering
\includegraphics[width=6in]{mds/figs/comb-4d.pdf} \\
\caption{The MDS projection of the domain in \fig{mds-comb} in four dimensions.}
\label{mds-comb-4d}
% generated by thesis/mds/figs/comb.R
\end{figure}

Although these plots are illustrative, a quantitative measure of how well the within-area distances are being approximated by the MDS projection is desirable. Given the eigen-decomposition attempts to minimize the spectral norm, this is the logical metric to use. The spectral norm may be calculated as the the square root of the largest eigenvalue of $\mathbf{D}-\mathbf{D}_\text{E}$ where $\mathbf{D}$ is the matrix of within-area distances (as above) and $\mathbf{D}_\text{E}$ is the matrix of Euclidean distances in MDS space.

% Results here are in mds/counter/run-DRcomp.R
Looking at this measure for the domain considered in this section (the ``comb''), peninsulae domain (from \secref{mds-wt2-sim}) and the Aral sea as dimension of projection is increased yields some interesting results. These are summarized in figure \ref{increasek}. The plot indicates that there is some optimum number of dimensions to project into, such that adding a further dimension gives only a negligible decrease in the spectral norm. 

The ``optimal'' projection dimension (in a spectral norm sense) is not common to all of the domains. For the peninsulae domain there is a large decrease (roughly halving each time) up to four dimensions, but the Aral sea appears to settle down after three. However, it's clear that the spectral norm is not the best guide for this given that for the ``comb'' domain, four dimensions appears to be optimal but \fig{mds-comb-3d} shows that this doesn't offer much separation in the peninsulae. The spectral norm doesn't offer a direct solution to the issue of dimension selection but it does at least offer the insight that there is some dimensional beyond which an increase in dimension only offers marginal returns, even if the best separation is after this point. Considering dimension selection separately from smoothing is sure to cause difficulties, since a point set that approximates the distances in $\mathbf{D}$ well does not give an guarantees about the quality of the resulting smooth. These dimension selection techniques only take into account variation in space, completely ignoring the effect that this has on the response. A method which takes into account how the projection affects the response will surely perform better than one which does not.

\begin{figure}
\includegraphics[width=\textwidth]{mds/figs/eigenplot.pdf} \\
\caption{Logarithm of the spectral norm of $\mathbf{D}-\mathbf{D}_E$ versus dimension for each of the domains detailed in \secref{nohigherdim}.\label{cor-r46}}
\label{increasek}
% generated by thesis/mds/figs/eigenplot.R
\end{figure}

Moving into continually higher dimensions is appealing, but practically it is rather more taxing. As the dimension of the problem is increased, the order of the derivative in the spline penalty increases too (see \secref{GAMpenalties} and \secref{gds-tprstoduchon}). As this happens, the dimension of the nullspace of the penalty increases, meaning that both the number and complexity of the unpenalized functions in the model increases. An increasingly large space of unpenalized functions is certainly unappealing, but the next chapter will investigate how to work around such issues.

\section{Conclusion}
\label{mds-conc}

This chapter has investigated the utility of using a combination of multidimensional scaling and penalized regression splines to combat the phenomenon of leakage in spatial smoothing. This was with a view to \mdsap\ being a less complex, faster, alternative to soap film smoothing, with an equally interesting motivating physical model.

The model certainly seems less complex. Provided that one knows about spline smoothing (which one would have to know to use the soap film smoother) and about multidimensional scaling (which is commonly taught at an undergraduate level outside of statistics, in biology, ecology, computer science, etc.), the method is relatively easy to get to grips with. No understanding of differential equations is required, nor is there any need to specify knots (unlike soap film smoothing). The physical model is still quite close to that which was outlined in \secref{intro-leakageapproaches} part 4: the idea of morphing the domain into a shape which does not suffer from leakage.

In comparison to using the \sch\ transform, \mdsap\ has the disadvantage of not being a functional mapping. Having a fixed functional form makes the \sch\ transform extremely fast since only a single function evaluation is needed to map a single point. In comparison, having to find the within-area distances is rather taxing computationally. However, the speed-ups presented in \secref{mds-faster} greatly enhance the utility of the method from a practical viewpoint. \mdsap\ and soap film smoothing differ in how their computational time is divided. The main computational burden of the soap film smoother is in solving the PDEs needed to form the basis and from that calculating the penalty matrix (see table \ref{wt2itimetable} and \secref{SF}), prediction is merely a case of evaluation. \mdsap\ on the other hand, spends the bulk of its computational time on finding the within-area distances for both fitting and prediction (comparing the ``thin plate'' and ``MDS+RS(\textit{pp})'' columns of table \ref{wt2itimetable}). The advantage of this is that since the distance calculation is effectively a black box from the perspective of the smoothing, if a faster routine for distance calculation were to be used there would be no difference in the results (provided that the routine calculated the distances exactly) but the computational time to calculate the final smooth would be significantly reduced. This approach also opens up the possibility of other distance metrics being used to form the distance matrix (as will be seen in \secref{gds-gds-examples}).

Figures \ref{wt2-3d-proj} and \ref{aral-pp} show that MDS does achieve the kind of domain morphing that was sought in section \ref{sc-conclusions}. Unlike the \sch\ transform, the projection does not force the points to move into a fixed transformation domain \label{cor-r47}and as such avoids some of the issues with point density. 

Using MDS to re-arrange the points does not entirely alleviate the problem of squashing. Although there is no numerical crowding (see \secref{sch-crowding}), points may still have an uneven spatial distribution, which causes problems for isotropic smoothers like the thin plate spline. \Secref{mds-penadjust} sought to avoid the problems that occur when the space in which smoothing is to be performed by adjusting the penalty based on the density of the points. This did not work because of the confounding issue of point ordering (as discussed in \secref{pensuck}). The varying point density and ordering problems were due entirely to using a low-dimensional MDS projection. Ensuring that point ordering is maintained and that the point density remains roughly even can be controlled by the projection dimension. Higher dimensional projections will be investigated further in the next chapter.

As we have seen over the last two chapters, although domain transformation methods are appealing from a mathematical and physical point of view, in practice they are tricky to apply and can produce artefacts in the resulting smooths. These artefacts can be avoided by projecting into higher dimensions where the ordering of the points is not disrupted. The potential problem of using higher dimensional projections is that model parsimony is jeopardized by an increasingly complex nullspace. In the next chapter the use of high dimensional MDS projections will be investigated when an appropriate spline basis can be used to perform smoothing. If reliable high dimensional smoothing can be performed, then the only remaining issue is to resolve the issue of projection dimension selection, the next chapter will deal with this too.
