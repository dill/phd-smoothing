% general distance smoothing stuff

\label{chap-gds}


\section{Introduction}

The previous chapter makes clear that we face a trade-off in using \mdsap. If the ordering of the points in space must be maintained, a sufficiently high dimensional projection of the data has been obtained. But increasing the projection dimension causes the nullspace of the penalty to become large, causing a large space of wiggly functions to be used without being penalised, leading to unreliable smoothing results. 

The requirement we seek is clear: we wish to smooth in as many dimensions as possible but while doing this we must also limit the nullspace of the penalty. Fortunately, this is possible using the ideas put forth in \cite{duchon77}. The methods detailed there give a generalisation of the thin plate spline basis that allows for the nullspace to be constrained.

The next section goes into the technical detail of how this approach works. Section \ref{gds-wad-examples} shows how this can be useful in the within-area distance case discussed in chapters \label{chap-sc} and \label{chap-mds}. Section \ref{gds-gds-examples} gives examples of generalized distance smoothing.

\section{From thin plate splines to Duchon splines}

\cite{simonbook} p. 153 gives the formulation for the thin plate penalty in $d$ dimensions with penalty order $m$ as:
\begin{equation}
J_{m,d} = \int \ldots \int_{\mathbb{R}^n} \sum_{\nu_1 + \dots + \nu_d=m} \frac{m!}{\nu_1! \dots \nu_d!}\Big( \frac{\partial^m f(x_1,\dots,x_d)}{\partial x_1^{\nu_1} \ldots  \partial x_d^{\nu_d}} \Big)^2 \text{d} x_1 \ldots  \text{d} x_d,
\label{tprs-pen}
\end{equation}
where the summation index can be interpreted as all of the possible combinations of penalty orders such than their sum is still $m$. The technical restriction $2m>d$ is also imposed to ensure that the function $f$ remains continuous.

The form of $f$ is:
\begin{equation}
f(\mathbf{x}) = \sum_{i=1}^n \delta_i \eta_{md}(\mathbf{x}-\mathbf{x_i}) + \sum_{j=1}^M \alpha_j \phi_j(\mathbf{x}),
\label{tprs-basis}
\end{equation}
(\cite{wood2003}) where the first summation is a series of radial basis functions, one per datum and the second summation are the unpenalised terms in the nullspace of the penalty. These are $M$ linearly independent polynomials of degree less than $m$. $M$ is given by:
\begin{equation*}
M=\begin{pmatrix} m+d-1 \\ d  \end{pmatrix}.
\end{equation*}
In the cases presented so far, $d$ (the MDS projection dimension) is known and $m$ is dictated by $d$. Increasing $d$ increases $m$ (by the continuity condition that $2m>d$) and hence increases $M$ in the manner above. Therefore $M$ increases very quickly with the number of dimensions used; this is shown in figure \ref{nullspace-dim} (blue line). As more functions are included in the nullspace, the more and more complex (because of their linear independence) they are. Having very wiggly functions in the nullspace is certainly undesirable, having many of them will be extremely problematic for model parsimony. 

\begin{figure}
\centering
\includegraphics[width=3in]{gds/figs/nullspace-dim.pdf} \\
\caption{Relationship between smoothing dimension ($d$) and the nullspace dimension ($M$) for thin plate regression splines (blue) and Duchon splines (red).}
\label{nullspace-dim}
% generated by thesis/mds/figs/nullspace-dim.R
\end{figure}

\cite{duchon77} begins with a much more general penalty than (\ref{tprs-pen}), a much more rigorous treatment is given in that paper; here is a more informal explanation of how from (\ref{tprs-pen}) one can reach the penalty given by Duchon.


The usual thin plate regression spline penalty given in (\ref{tprs-pen}) can be thought of as a measure of ``wigglyness'' or ``roughness'', taking higher values when $f$ is particularly rough or wiggly (an indication that the spline is attempting to interpolate). Integrating the square of the derivatives over the whole of $\mathbb{R}^d$ penalises the fitting such that the optimisation alters the parameters in a way that reduces the wigglyness of the whole function (TKTKTK come back and re-write this sentance).


Duchon's proposal is to change the penalty so instead of integrating the squared derivatives, the squared Fourier transform of the derivatives is integrated. The Fourier transform decomposes functions defined in space into their frequency domain representations (from $\mathbf{x}$ to $\boldsymbol{\tau}$). This is analogous to finding the Fourier series approximating $g$ (but in the limit such that the sum is no longer approximate). One can also think of the Fourier transform as a change of basis. Mathematically, the Fourier transform of some function, $g$, is defined as:
\begin{equation*}
\mathfrak{F} g(\boldsymbol{\tau}) = \int \ldots \int_{\mathbb{R}^n} e^{2 \pi \sqrt{-1} \mathbf{x}^\text{T} \boldsymbol{\tau}} g(\mathbf{x}) \text{d}\mathbf{x}.
\end{equation*}
Here $\mathfrak{F}$ is an operator applied to $g$, so $\mathfrak{F}g$ may be considered as a function of $\boldsymbol{\tau}$ (an $n$-vector which characterizes the frequencies). More detail on Fourier transforms can be found in (for example) \cite{bracewell} and \cite{beerends}. 

TKTKTK COME BACK TO THIS!!!

Taking the Fourier transform of the penalty allows us to think of the quantity is calculated in a different way. Rather than thinking about measuring the wigglyness over the whole of $f$, point by point, instead the measure is based on the frequencies of $f$, the higher the frequency components contribute more to the integral than the lower ones. Intuitively this makes sense, since the low frequency components of $f$ are likely performing a similar task to those functions in the nullspace of the penalty, where as the more complicated, high frequency components are more likely to represent $f$ attempting to interpolate parts of the smooth when dissimilar values are close together. (TKTKTK add more here)

Mathematically, this penalty can be written:
\begin{equation}
J_{m,d} = \int \ldots \int_{\mathbb{R}^d} \sum_{\nu_1 + \dots + \nu_d=m} \frac{m!}{\nu_1! \dots \nu_d!}\Big( \mathfrak{F} \frac{\partial^m f}{\partial x_1^{\nu_1} \ldots  \partial x_d^{\nu_d}}(\boldsymbol{\tau}) \Big)^2 \text{d} \boldsymbol{\tau}.
\label{tprs-pen-ft}
\end{equation}

The penalties, (\ref{tprs-pen}) and (\ref{tprs-pen-ft}) are in fact equivalent by the Plancherel theorem (TKTKTK cite Plancherel), so one can take either view and obtain the same results when smoothing. 

Since this alternative, frequency interpretation appears to be inuitive and gives the same results as the conventional penalty, we could extend it to a more general class of penalties by putting a weighting in the integral:
\begin{equation*}
\breve{J}_{m,d} = \int \ldots \int_{\mathbb{R}^d} w(\boldsymbol{\tau}) \sum_{\nu_1 + \dots + \nu_d=m} \frac{m!}{\nu_1! \dots \nu_d!}\Big( \mathfrak{F} \frac{\partial^m f}{\partial x_1^{\nu_1} \ldots  \partial x_d^{\nu_d}}(\boldsymbol{\tau}) \Big)^2 \text{d} \boldsymbol{\tau},
\end{equation*}
the function $w$ could then be used to pick out particularly high frequencies and penalise those more than the lower frequency ones. It is also possible to recover (\ref{tprs-pen}) by simply setting $w(\boldsymbol{\tau})=1, \forall \boldsymbol{\tau}$.

Duchon shows that if $w(\boldsymbol{\tau})= \lvert \boldsymbol{\tau} \rvert^{2s}$ is used then the thin plate spline functions in \ref{tprs-basis} arise. In this case the penalty is:
\begin{equation}
\breve{J}_{m,d} = \int \ldots \int_{\mathbb{R}^d} \lvert \boldsymbol{\tau} \rvert^{2s} \sum_{\nu_1 + \dots + \nu_d=m} \frac{m!}{\nu_1! \dots \nu_d!}\Big( \mathfrak{F} \frac{\partial^m f}{\partial x_1^{\nu_1} \ldots  \partial x_d^{\nu_d}}(\boldsymbol{\tau}) \Big)^2 \text{d} \boldsymbol{\tau},
\label{duchon-penalty}
\end{equation}
again, (\ref{tprs-pen}) can be recovered from this penalty (by setting $s=0$). When $s>0$ higher frequencies are penalised more than lower ones. In order to obtain smooth functions it is required that $m+s>d/2$. So $s$ can be used to allow high dimensional smoothing, while still using lower-order penalties without yielding discontinuous functions. One can therefore think of $s$ as a kind of ``fudge factor'' that allows the conditions on $m$ and $d$ to be relaxed; given some fixed combination of $m$ and $d$, an $s$ can be found (by simply calculating $s=d/2-m$).

Looking back to figure \ref{nullspace-dim} (where the blue line showed the increasing numbers of functions in the nullspace of the conventional thin plate penalty), the red line gives the number of functions in the nullspace of the penalty in (\ref{duchon-penalty}). In this case $m=2$ has been chosen to remain constant as the dimensionality increases.

When using MDS TKTKTK something something manifolds something\ldots 
\ldots In any small region a 2-dimensional manifold will look like $\mathbb{R}^2$, in the same small region the smoother's behaviour will closely approxiate a regular thin plate regression spline with $m=2$ on the tangent space of the manifold (as the smoothing parameter tends to zero, at least).




\subsection{Duchon-type penalties elsewhere}
SAY SOMETHING ABOUT \cite{girosi} TKTKTK add page numbers to ref




Sum up










\section{Choosing MDS projection dimension}

With Duchon's basis in mind, it is now possible to smooth a space of any dimension using an order 2 penalty, simply by picking $s$ appropriately. Picking the dimension of the MDS projection is now a concern since there is no reason to believe that simply going to higher and higher dimensions will yield better and better results. Keeping in mind the desire for a simple model, it would be preferable to keep the dimension of the projection as small as possible.

\subsection{Using proportion of variation explained}

It is clear that as higher and higher projections are used, a larger and larger proportion of the variation in the distance matrix is explained. It therefore seems reasonable to base the choice of dimension on the proportion of the variation explained in the initial grid. However, what proportion should be used? 80\%? 90\%? 99\%? There is no reason to choose any one of these over the others. Setting the proportion of variation to be explained generally is surely a bad idea, since what works for one domain may well be a disaster for another.


\subsection{Using GCV scores}

Since fitting a GAM with a Duchon spline basis is relatively cheap compared to the cost of finding the within-area distances, the GCV score can be used to find an optimal dimension for the MDS projection. So, using this to our advantage, a series of projections can be tried, their GCV scores calculated, and the best selected as the projection to use for the model.

In particular, starting from a 2-dimensional projection, the dimensionality is increased  and models fitted, until there i an increase in GCV score. The upper bound is the number of dimensions that explain 95\% of the variation in the distance matrix of the initial grid (see section \ref{mds-prac}).  This favours simpler models (in dimensionality terms), although of course a full search can be performed if there is some prior belief that the dimensionality should be higher (indeed there is no reason to believe that the GCV score should be unimodal in dimension). Simulations show that the minima in the GCV score and MSE are in agreement.

Figure \ref{wt2-gcv-projdim-boxplot} shows a plot of GCV score and mean squared error for 60 simulations from the peninsula domain, for each of the 60 realisations a [[new NAME]] was fitted using a 2 through 20 dimensional projection. The boxplots are grouped according to the dimension of the MDS projection used. The graph shows that there is a minima in the score when the dimension is four which corresponds with the minimum MSE.

\begin{figure}
\centering
\includegraphics[width=6in]{mds/figs/wt2-gcv-projdim-boxplot.pdf} \\
\caption{MSE and GCV score for the wiggly top domain when different dimensional projections are used. Here a 4-dimensional projection gives a minimum GCV score and MSE.}
\label{wt2-gcv-projdim-boxplot}
% generated by mds/duchon/wt2-gcvml.R
\end{figure}

There is, of course, no guarantee that there will always be a minimum to find and as with all automated methods, practitioners should be mindfull of what could potentially go wrong. However, in the situations tested, this method appeared to be satisfactory.

\section{Within-area distance examples}
\label{gds-wad-examples}
\subsection{Simulations}

Returning to the simulation study in section \ref{wt2bigsim}, the same set of simulations (250 samples at three error levels) was re-run but now using the Duchon splines, selecting MDS dimension based on GCV score (maximum basis dimension was set at 100). The results are shown for the original simulations along side those for the new model (those models which did not to very well originally are excluded).

Figure \ref{wt2-boxplot-duchon} shows the boxplots for these simulations, the [[GIVE THIS A NAME]] method does better than the soap film smoother when the noise is low and was indistinguishable (via a paired Wilcoxon signed rank test) from the soap film smoother at higher noise levels.

\begin{figure}
\centering
\includegraphics[width=6in]{mds/figs/wt2-boxplot-duchon.pdf} \\
\caption{Boxplots of logarithm of per realisation average mean squared error from simulations on the wiggly top domain. The boxplots are in groups of four for each error level $(\sigma = 0.35, 0.9, 1.55)$. Colours indicate the result of a Wilcoxon paired signed rank test of whether the MSE was significantly (p$<10^{-2}$) different from the soap film smoother. Red indicates different and worse, green different and better.}
\label{wt2-boxplot-duchon}
% generated by mds/duchon/bigsim/boxplot-wt2.R
\end{figure}

[[new wt2??]]

[[aral sim as before?]]

[[ramsay??]]



\subsection{Revisiting the Aral sea}

Going back to the Aral sea example from section \ref{aral-sec}, we can fit a model using the optimal (in a GCV score sense) dimension. Figure \ref{mds-aral-5d-duchon} shows the raw data and a smoothed version, using a 5-dimensional projection (given by minimising the GCV score). The plot does not contain any of the artefacts that were present in the previous smooths of the data.

\begin{figure}
\centering
\includegraphics[width=6in]{mds/figs/aral-5d-duchon.pdf} \\
\caption{The Aral sea chlorophyl levels smoothed using Duchon splines, when a 5-dimensional MDS projection is employed. Note the lack of artefacts in comparison to previous \mdsap\ models.}
\label{mds-aral-5d-duchon}
% generated by duchon/aral-evolve.R
\end{figure}



\section{Generalized distance smoothing}
\label{gds-gds-examples}

Duchon splines are able to smooth in high dimensions without the usual problems that come from such situations. Multidimensional scaling allows the projection of any arbitrary distance matrix [TKTKTK check] into Euclidean space. The combination of these two methods has been sucessful in the within-area distance case above, but also allows for smoothing using any set of distances.

Data are often collected on scales that are not necessarily physically meaningful (for example in psychological studies or attitude surveys) but the data are used as if the scale was absolute in some sense. In such cases the distances between the observations may be meaningful but not the actualy ordinal values.

Taking data which is either already distances or distances calculated from data, the MDS projection can be found and then a smooth over those data can be used to model some response.

% concrete example of the above

% measurement error

%
\subsection{Examples}

\textbf{SOME BLURB HERE}


\subsubsection{MP data}

% talk about:
%  how many labour MPs
%  confounding with Plaid/SDLP etc

The website Public Whip (\url{http://www.publicwhip.org.uk/}) provide data from the Hansard on the votes of 676 MPs in the 1997--2001 Westminster parliament. During this time the House took 1273 divisions. Each vote is coded according to table \ref{voting-code}. Also available are the party allegiances of each MP. 


\begin{table}  
\begin{centering}
\begin{tabular}{ccc}
    Value & Description & Code \\ 
    \hline
    Missing & MP did not vote in this division & 0 \\ 
    Tell aye & MP voted for the motion and was a teller & 1 \\ 
    Aye & MP voted for the motion & 1 \\ 
    Both & MP voted both for  and against the motion & 0 \\ 
    No & MP voted against the motion & -1 \\ 
    Tell no & MP voted against the motion and was a teller & -1 \\ 
  \end{tabular}
\caption{Coding of UK MP voting data. For the purposes of the analysis here the teller's votes are counted as if they voted since we are interested in how voting can be used to predict part affiliation. Note that ``both'' is perfectly possible, and occurs when the MP walks through both the ``Aye'' and ``No'' gates, this can correspond to the MP abstaining (as with ``Missing'') or no nullify a mis-cast vote.}
\end{centering}
\label{voting-code}
\end{table}

Taking the matrix of votes, Euclidean distances between the MPs (rows) were found and used to form the distance matrix. Multidimensional scaling was then used to project these distances into MDS space (dimension selection was performed by optimizing the GCV or ML score). The party affiliation (Labour versus not Labour) was then predicted using a logit link.

Of the 1273 divisions in the 1997--2001 parliament, 17 of them were declared  as ``free votes'' (\cite{freevotes}), where MPs are not ``whipped'' (pressured to take the party line). These 17 votes are used since classifying based on all votes makes the classification very easy (most MPs do what they're told, most of the time). The free votes are summarised in table \ref{free-vote-description}; most of these are ``conscience'' votes.

\begin{table}  
\begin{centering}
\begin{tabular}{cc}
	Date & Bill name \\
    \hline
22 March 2001    &   Election of a Speaker \\
17 January 2001  &   Hunting Bill \\
17 January 2001  &   Hunting Bill \\
17 January 2001  &   Hunting Bill\\
20 December 2000 &   Hunting Bill \\
19 December 2000 &   Human Fertilisation and Embryology\\
31 October 2000  &   Stem Cell Research \\
14 April 2000    &   Medical Treatment (Prevention of Euthanasia) Bill \\
28 February 2000 &   Sexual Offences (Amendment) Bill \\
10 February 2000 &   Sexual Offences (Amendment) Bill \\
28 January 2000  &   Medical Treatment (Prevention of Euthanasia) Bill\\
25 January 1999  &   Sexual Offences (Amendment) Bill\\
22 June 1998     &  Crime and Disorder Bill \\
22 June 1998     &  Crime and Disorder Bill \\
28 November 1997 &  Wild Mammals (Hunting with Dogs) Bill\\
  \end{tabular}
\caption{Free votes in the 1997-2001 parliament (see \cite{freevotes}).}
\end{centering}
\label{free-vote-description}
\end{table}

Using these votes, the


Using these votes, 100 MPs were selected at random and the model fit to the data and predictions were then made over all 676 MPs. For comparison two implementations of the lasso (using the packages \texttt{lars} and \texttt{glmnet}) were fit one with identity link and the other with a logit link.

Over realisation 200 times the number of misclassifications as well as which MPs were misclassified were recorded for the models. The GCV score at different numbers of dimensions was also recorded for the MDS+DS model.




\subsubsection{Breast cancer microarrays}




\subsubsection{Leukemia microarrays}



\section{Software implementation - \mdspack}

The methods detailed in this section, namely the use of Duchon splines along with MDS to perform smoothing are provided in the \textsf{R} package \mdspack\ which is available at A WEBSITE THAT MIGHT BE CRAN.



