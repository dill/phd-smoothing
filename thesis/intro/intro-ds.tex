\label{chap-intro-ds}

\label{intro-DS}

Distance sampling (\cite{IDS}, \cite{ADS}) is a popular method for estimating the abundance of biological populations. It has been used by researchers across the globe to assess the abundance of everything from birds nests to marine mammals. Surveys are cheap to run since they do not require many observers (unlike a census) or multiple site visits (unlike mark-recapture). Distance sampling is also rather different from methods like mark-recapture (\cite{ruthbook}) as it does not explicitly include the abundance in the likelihood, as shall be seen below. The popularity of distance sampling is in part due to the software Distance (\cite{distance-software}) which makes it easy to record and analyse distance sampling data. 

\section{From quadrat sampling to distance sampling}
\label{quad2ds}

One can think of distance sampling as the logical extension of quadrat and strip transect sampling. In quadrat sampling a series of squares (quadrats) are laid out at random over the sample area and the number of objects of interest within each is counted. It is assumed that within each quadrat a census is performed. From the per-quadrat abundance the density is estimated and multiplied-up to find the total abundance. For quadrat sampling to be efficient the quadrats need to be large and hence it is almost impossible to ensure that all objects in the quadrat are seen, this can be further hindered by animals moving between the quadrats during the survey (\cite[p. 2]{IDS}).

% evolution to DS from quadrat and strip
\begin{figure}
\centering
% trim order l b r t
\includegraphics{intro/figs/quadrat-to-ds.pdf}\\
\caption{An example of quadrat sampling (left), strip transect sampling (middle), and distance sampling (right). Dots indicate individuals, red dots are observed individuals, black those missed. In the first two cases, the grey boxes represent the sampling units. Note that there are many observations just outside of the boxes, which cannot be recorded by survey staff. In the distance sampling case, the solid vertical lines represent the transects and the dashed line gives the effective strip width. Distances are shown by the solid horizontal lines.}
\label{quad-to-ds}
\end{figure}

To make the task of counting the objects within the quadrat easier, one could modify the square design to be a long strip, so that the observer could walk down the centreline of the strip, observing those objects within the strip. Mathematically, if we let the each strip be of width $2w$ ($w$ either side of the line the observer walks down) and the sum of all strip lengths be $L$, if $n$ objects are observed we have a simple estimator of the density, $D$:
\begin{equation}
\hat{D}=\frac{n}{2wL}.
\label{ds-simpleD}
\end{equation}

The problem with both quadrat and strip sampling is that there may well be many objects just outside of the covered area. Clearly this is a waste of survey effort, since observers must ignore objects that they have seen but that are not within the strip. It would be preferable to include as many observations as possible and leverage the maximum amount of data that can be collected to assess the abundance of the population.

Distance sampling is based on this principle; if the objects of interest are seen, then their presence should be recorded. Instead of using fixed-area sampling units, distance sampling requires that only centrelines are specified. The observers should walk (or swim, ride, drive, sail, etc) down the centrelines recording the distances ($x_i$) to the observed objects as they go. Once the survey is complete, the distances are used to estimated the effective area that was sampled. Figure \ref{quad-to-ds} shows the evolution from quadrat to strip to distance sampling.

In equation (\ref{ds-simpleD}) one can think of replacing $w$ with an estimate of $\mu$, the \textit{effective strip (half-)width}. This is the distance at which as many animals were detected beyond as there were missed inside. Further explanation of $\mu$ is given in \secref{gtoD}, but it serves for now to say that by replacing $w$ with $\mu$ an estimate of the area that was effectively surveyed can be found. (\ref{ds-simpleD}) can then be modified to:
\begin{equation}
\hat{D}=\frac{n}{2\hat{\mu}L}.
\label{ds-D}
\end{equation}
We could also consider that a certain proportion ($\hat{p}$, the probability of detection) of the objects in a fixed-area ($2wL$) were sampled, so the above can also be expressed as:
\begin{equation*}
\hat{D}=\frac{n}{2wL\hat{p}}.
\end{equation*}
Here $w$ is the point after which observations are discarded and is referred to as the \textit{truncation} distance. Truncation is used to discard outliers that make the estimation process tricky (\cite[pp. 15-16]{IDS}). From these two expressions we can see that the relationship between $p$ and $\mu$ is $p=\mu/w$, these quantities will be investigated further below.

A typical example of line transect data is shown in figure \ref{ds-lt-example}. The figure shows a histogram of perpendicular distances. Note how, as distance increases the number of detections decreases. This characteristic will be exploited later.

% example of line transect data
\begin{figure}
\centering
\includegraphics{intro/figs/ds-golftee.pdf}\\
\caption{A histogram of line transect data. In this case from an experiment conducted at the University of St Andrews. 760 golf tees were randomly distributed over a 1680m$^2$ area, then observed in 11 transects by 8 independent surveys. Further detail may be found in \citeb[p. 140]{ADS} and \citeb{yellowbook}.}
\label{ds-lt-example}
\end{figure}

\subsection{Point transects}
Line transects are not the only way of collecting data for a distance sampling analysis; point transects may also be used. When using point transects the observer stands at one of a series ($m$, say) of points and observes the objects surrounding him/her. Again, distances to the objects ($r_i$) are recorded. An \textit{effective radius} ($\rho$) is then calculated (analogously to $\mu$) and then object density can be estimated by\label{cor-7s1}:
\begin{equation*}
\hat{D}=\frac{n}{m \pi \hat{\rho}^2}=\frac{n}{m\pi w^2\hat{p}}.
\end{equation*}
The relation between these quantities will be explained below in \secref{gtoD}.

An example of point transect data is given in figure \ref{ds-pt-example}. In contrast to the line transect case, there are very few observations near 0, they increase to a point and then fall off beyond that. Note that as the distance, $r$, from the observation point increases the area surveyed increases as $r^2$. Rescaling this histogram by the distance to the midpoints of each bin will give a histogram that has a similar shape to that of figure \ref{ds-lt-example} (i.e. the rescaling accounts for the increasing area available to the observer as the distance from the point increases).

% example of point transect data
\begin{figure}
\centering
\includegraphics{intro/figs/pt-data-example.pdf}\\
\caption{A histogram of point transect data of Hawaiian amakihi (\textit{Hemignathus virens}) taken from \citeb{amakihi}.}
\label{ds-pt-example}
\end{figure}


\section{Assumptions}
\label{ds-assumptions}
\label{cor-7s2}In order to ensure that estimation is unbiased several assumptions are made. It is first assumed that the objects are distributed throughout the survey region according to some stochastic process. Line or point placement must be random with respect to the distribution of objects; given that this is done, it can be safely assumed that the objects are distributed uniformly (i.e. the process is stationary; see \citeb[p. 49]{ADS}) from the point or line. (\cite[p.  29]{IDS})\label{cor-7s12}. 

In order to obtain reliable estimates of the density from the point or line, the following three assumptions must hold:
\begin{itemize}
	\item Objects on the line (or point) are detected with probability 1.
	\item Objects are observed in their initial location, not after movement in response to the observer.
	\item The recorded distances are accurate.
\end{itemize}
Further assumptions regarding field procedure may be found in \citeb{IDS}, chapter 2. 

\section{The detection function}
\label{intro-ds-detfct}

One would expect that the probability of observing an object\label{cor-7s3} would decrease as the distance from the observer increased (as in figure \ref{ds-lt-example}\label{cor-7s4}). This is the relationship captured by the detection function ($g(x)$) which is defined as (\cite[p. 10]{IDS}):
\begin{equation}
g(x)=\mathbb{P} (\text{object detected} | \text{object was at distance } x).
\label{ds-wordyeqn}
\end{equation}
The goal in distance sampling is to accurately model the detection function and through this estimate the effective strip width (or effective radius), see \secref{gtoD} below. Before talking about models for $g(x)$, we look at the desirable properties.

First, as stated in the assumptions above, objects on the line (or point) are detected with certainty, so $g(0)=1$. Second, it is preferable to have a model where detection is almost certain near zero distance, i.e. that the function has a \textit{shoulder}. This is physically realistic since the observer should see most things close to him/her (not just those directly under/in front of him/her). Third, it is also desirable that the model for the detection function is robust, in the sense that it is a general, flexible model that can take many plausible shapes (see \cite[p. 41]{IDS}). Finally, it is desirable to have a model that is efficient, in the sense that estimates have a relatively small variance, however this is only of use when the other criteria are met.

\label{cor-7s5}\citeb{buckland92} gives a ``key function plus adjustment terms'' formulation for the detection function. In this formulation the key function (denoted $k$) is used as a starting point for the basic shape of the detection function (it has certain detection at zero distance and has a shoulder). The adjustment terms (each denoted $s_j$) consist of a series expansion that improve the fit of the model. The model is written as:
\begin{equation*}
g(x; \bm{\theta}) = \frac{k(x; \bm{\theta}_k) \{1+\sum_j s_j(x; \bm{\theta}_s)\}}{k(0; \bm{\theta}_k) \{1+ \sum_j s_j(0; \bm{\theta}_s)\}},
\end{equation*}
where $\bm{\theta}$ is a vector of all of the parameters ($\bm{\theta}=(\bm{\theta}_k,\bm{\theta}_s)$) and the index of summation depends on the form of adjustments. The denominator ensures the detection function is 1 at zero distance. 

The key function, $k$, is usually selected as one of:
\begin{align*}
k(x,\bm{\theta}_k) = \begin{cases} 1/w &\qquad \text{(uniform)}\\
\exp \left( -\frac{x^2}{2\sigma^2}\right) &\qquad \text{(half-normal)}\\
1- \exp \left( -\left[ \frac{x}{\sigma}\right]^b\right) &\qquad \text{(hazard-rate)}.
\label{ds-detfct}
\end{cases}
\end{align*}
In each of the above cases $\bm{\theta}$, the parameter(s) to estimate is/are: non-existant (no parameters are estimated for uniform key), $\sigma$ (known as the \textit{scale parameter} and $(\sigma,b)$ respectively ($b$ is known as the \textit{shape parameter}). The adjustment terms are then one of:
\begin{equation*}
s_j(x,\bm{\theta}) = \begin{cases} 0 & \qquad \forall j \text{ (no adjustments)}\\
a_j \left( \frac{x}{w}\right)^{2j} &\qquad j=1,\dots,J \text{ (simple polynomial)}\\
a_j \cos\left( \frac{j \pi x}{w}\right) & \qquad j=2,\dots,J \text{ (cosine)}\\
a_j H_{2j}(x)& \qquad j=2,\dots,J \text{ (Hermite polynomial)}.\\
\end{cases}
\end{equation*}
In each case we wish to estimate the $a_j$s (so $\bm{\theta}$ is a vector of $a_j$s). More information on the formulation can be found in \citeb[p. 47--48]{IDS} and \citeb{buckland85}. Figure \ref{ds-detfct-examples} shows some possible detection functions. 

% Possible detection functions
\begin{figure}
\centering
\includegraphics{intro/figs/detfct-examples.pdf}\\
\caption{Three possible detection functions. The first is a half-normal distribution, the second a hazard-rate function and the third the same half-normal as the first but with a cosine adjustment term. The hazard-rate function has a controllable ``shoulder''.}
\label{ds-detfct-examples}
\end{figure}

This formulation leads to a class of highly flexible models. Data analysis usually consists of running models made up of various combinations of key functions and adjustment terms. Model parameters are found using maximum likelihood (see \secref{gtoD}). Model selection is performed via AIC (\cite[p. 69]{IDS}), that is:
\begin{equation}
\text{AIC} = -2 \hat{l} + 2P,
\label{DEFN-AIC}
\end{equation}
where $\hat{l}$ is the value of the log-likelihood at the MLE and $P$ is the number of parameters in the model. Models with many parameters are penalized more heavily than those which are more parsimonious.\label{cor-5s8}

\section{From $g(x)$ to $D$}
\label{gtoD}
Equation (\ref{ds-D}), above shows that in order to estimate the density of the population in question, we must find an estimator of $\mu$, the effective strip width (or, equivalently for point transects, $\rho$). To do this the detection function is used.

\subsection{Line transects} 
As mentioned above, $\mu$ is defined to be the distance from the lines for which as many objects are detected beyond $\mu$ as are missed within $\mu$ (\cite{eenviron}). Looking at figure \ref{ds-mu-explanation}, the shaded area above the detection function represents those objects that the observer missed up to a distance $\mu$ from the line. The shaded area below the curve represents those objects that were observed beyond a distance $\mu$. By moving $\mu$ to the left or to the right, it is possible to find a point at which the two shaded areas are of equal size. This fulfils the criterion for $\mu$.

% explanation of mu
\begin{figure}
\centering
\includegraphics{intro/figs/muexplanation.pdf}\\
\caption{A detection function $g(x)$ with the effective strip width $\mu$ marked as well as the truncation distance $w$. The shaded regions have equal area, this means that the area under the curve has the same size as the rectangle with base length $\mu$. Figure taken from \citeb{IDS}.\label{cor-7s6}}
\label{ds-mu-explanation}
\end{figure}

Now the question is: how is $\mu$ calculated and how does this relate to the detection function? Note that the rectangle with side $0$ to $1$ on the $y$ axis and $0$ to $\mu$ on the $x$ axis has area $\mu$ and that this is the same as the area under the detection function (by the argument above). So $\mu$ can be defined as:
\begin{equation}
\mu = \int_0^w g(x) \text{d}x,
\label{ds-lt-mu-def}
\end{equation}
where $w$ is again the truncation distance and ignoring the exact form of $g(x)$. Hence $p$ is defined as:
\begin{equation*}
p = \frac{\int_0^w g(x) \text{d}x}{w}.
\end{equation*}
\label{cor-r59}So, an estimator of density may be written as:
\begin{equation*}
\hat{D}=\frac{n}{2 w\hat{p} L}=\frac{n}{2\hat{\mu}L}.
\end{equation*}
That is, the density is estimated by the number of observations divided by the effective area that was surveyed. Since $\hat{\mu}$ gives the distance to which as many objects were observed as missed, the area can be treated like a strip transect where everything was observed out to $\hat{\mu}$, the strip is $2\hat{\mu}$ wide (since we are only estimating the half-width), and $L$ long.

The detection function is one of the functions described in \secref{intro-ds-detfct} and so only its parameter(s) must be estimated. However to first form the likelihood, the probability density function of the distances\label{cor-r59-1} must be defined. Note that the expected number of objects at a distance $x$ from the transect line (including those not observed) is independent of $x$. This then implies (for line transects) that the shape of the density functions is the same as that of the detection function and can therefore be obtained by rescaling (\cite[p. 38]{IDS}). So, the PDF of the perpendicular distance data, conditional on the object being observed is then:
\begin{equation*}
f(x;\bm{\theta}) = \frac{g(x;\bm{\theta})}{\int_0^w g(x;\bm{\theta}) \text{d}x} = \frac{g(x;\bm{\theta})}{\mu},
\end{equation*}
where $\bm{\theta}$ is a vector of all of the parameters of $g$.

Now we have obtained an expression for the PDF, we can form a likelihood:
\begin{align}
\mathcal{L}(\bm{\theta}; \bm{x}) &= \prod_{i=1}^n f(x_i;\bm{\theta}),\notag \\
&= \prod_{i=1}^n \frac{g(x_i;\bm{\theta})}{\mu},
\label{ds-lt-likelihood}
\end{align}\label{cor-7s7}
where $\bm{x}$ is an $n$-vector of the observed distances.

The log-likelihood can then be used as an objective function in an optimization procedure in order to find the maximum likelihood estimators of the parameters.

\subsection{Point transects} 

For point transects, instead\label{cor-7s8} of effective strip width, we look at effective radius. This effective radius, $\rho$ is defined in the same way as $\mu$: that there are as many objects missed up to $\rho$ as observed beyond. Related to the effective radius is the \textit{effective area of detection}, $\nu=\pi \rho^2$.

For line transects, an infinitesimal strip has area $l\text{d}x$ (the length of the line multiplied by an infinitesimal distance perpendicular to the line) and this value is independent of $x$. However, in the point transect case an incremental annulus depends on $r$ (the distance from the point), such an annulus has area $2\pi r \text{d}r$. So, the effective area of detection is therefore defined as:
\begin{equation*}
\nu = 2 \pi \int_0^w r g(r) \text{d}r.
\end{equation*}
Then, following through the arguments above, we can define the PDF as:
\begin{equation*}
f(r) = \frac{r g(r)}{\int_0^w r g(r) \text{d}r},
\end{equation*}
and hence: 
\begin{equation*}
f(r) = \frac{2 \pi r g(r)}{\nu}.
\end{equation*}
A more rigorous proof of this is given in \citeb[p. 54]{IDS}.

By analogy to the line transect case, the relation between $\nu$ and $p$ in the point transect case is
\begin{equation*}
p=\frac{\nu}{\pi w^2},
\end{equation*}
so:
\begin{equation*}
f(r) = \frac{2 \pi r g(r)}{\pi w^2 p}.
\end{equation*}
Again, a likelihood can be formed:
\begin{align*}
\mathcal{L}(\bm{\theta}; \bm{r}) &= \prod_{i=1}^n f(r_i;\bm{\theta}),\\
&= \prod_{i=1}^n \frac{2 \pi r_i g(r_i;\bm{\theta})}{\nu},
\end{align*}
where $\bm{r}$ is an $n$-vector of the observed distances. 

As above, the log of this expression can then be used in an optimization procedure to find the MLEs of the parameters.

\section{Multiple covariate distance sampling}
\label{intro-ds-covar}

One can very easily think of the case in which one or more factors (as well as distance) affect the detectability of objects in the survey. A typical example might be the sex or size of the object or weather conditions at the time of observation. When available this information can be very useful in handling heterogeneity in the detectability (\cite[p. 88]{IDS}). Covariates are included in detection function models by using a link function (\cite{davidbthesis}; \cite{covpaper}\label{cor-7s10-2}\label{cor-8s3-1}). Figure \ref{ds-covarex} shows the effect of both continuous and factor covariates on a hazard-rate detection function.

\label{cor-r62}Non-covariate distance sampling, as in the previous section, is commonly referred to as CDS (conventional distance sampling) and covariate distance sampling as MCDS (multiple covariate distance sampling).

% example covariates and how they effect the detection function
\begin{figure}
\centering
% trim order l b r t
\includegraphics{intro/figs/amakihi-detfct.pdf}\\
\caption{The influence of covariates on the detection function (taken from \cite{amakihi}). The detection functions are for Hawaiian amakihi (\textit{Hemignathus virens}). The left panel shows the effect of a factor covariate (observer), while the right shows the effect of a continuous covariate (time). In each case the other covariate is held constant (time at $0900$ and observer as ``TJS'', respectively) while the other is varied.}
\label{ds-covarex}
\end{figure}

\label{cor-7s10}\label{cor-8s3-2}Referring back to the detection functions given in \secref{intro-ds-detfct}, the covariates affect the scale parameters ($\sigma$) of the hazard-rate and half-normal detection functions only. It is also assumed that the distributions of the distances and covariates are independent (\cite{covpaper}). Since the distribution of the covariates is unknown, the conditional distribution of the distances, given the observed covariates is modelled.

\label{cor-7s9}\label{cor-r60}Say that for observation $i$ a set of $K$ covariates are collected and denote them store them in the $K$-vector $\mathbf{z}$ (containing $z_{i1}, \dots, z_{iK}$). The definition of the detection function in (\ref{ds-wordyeqn}) is now:
\begin{equation*}
g(x,\mathbf{z})=\mathbb{P} \left(\text{object detected} \vert \text{object was at distance } x \text{ with covariates } \mathbf{z} \right).
\end{equation*}
So then defining the scale parameter on a per-observation basis, we have:
\begin{equation}
\sigma_{i} = \exp( \beta_{0} + \sum_{k=1}^K \beta_k z_{ik}).
\label{intro-ds-covar-model}
\end{equation}
Now, rather than estimating the scale parameter, we estimate the $\beta_k$s. This covariate formulation can be thought of as a generalisation of the non-covariate model, the latter simply being the case where there is only an intercept term. Note that it may be necessary to standardize the distances by dividing them by either the scale parameter or the truncation distance, guidance on when to use which standardization is given in \cite{covpaper}.

This formulation fits nicely into the likelihood expressions above (although note that for line transects $f(x_i;\bm{\theta})$, is replaced by $f(x_i\vert\mathbf{z}_i; \bm{\theta})$ and similarly for point transects), the evaluations of the detection function are as above, but the calculation of $\mu$ (and therefore $p$) changes. 


%The for each observation, $i$, the corresponding covariates, $\mathbf{z}_i$, are stored as rows of the $n \cross K$ matrix $\mathbf{Z}$.


%Now, say that two covariates have been collected, $z_{i,\text{sex}}$ and $z_{i,\text{Beau}}$ indicating the sex of animal $i$ and the Beaufort sea state (which measures the roughness of the sea) when animal $i$ was observed. Also recorded are the distances $x_i$. Covariates can be included in the analysis by considering $\sigma$ as a linear combination of these covariates:
%\begin{equation}
%\sigma_i = \exp( \beta_0 + \beta_1 z_{i,\text{sex}} + \beta_2 z_{i,\text{Beau}}).
%\label{intro-ds-beaufort-covar-model}
%\end{equation}
%The objective is now the estimation of $(\beta_0, \beta_1, \beta_2)$ (again, by maximum likelihood).


The effective strip width, $\mu_i$ is now expressed as:
\begin{equation}
\mu_i = \int_0^w g(x ; \sigma_i) \text{d}x,
\label{intro-ds-mu-covar}
\end{equation}
that is, that the effective strip width depends on the covariate values. So in the covariate case there is no single probability of detection, instead the per observation (equivalently, per unique covariate combination) probability of detection can be calculated for line transects:
\begin{equation*}
p_i = \frac{\int_0^w g(x ; \sigma_{i}) \text{d}x}{w}.
\end{equation*}
For point transects, the probability of detection and effective area are given as:
\begin{equation*}
p_i =\frac{2}{w^2}\int_0^w r g(r; \sigma_i) \text{d}r, \qquad \nu_i = 2\pi \int_0^w r g(r; \sigma_i) \text{d}r.
\end{equation*}
\label{cor-7s11}

\subsection{Estimating population size}
\label{intro-ds-pop-size}
\label{cor-r61}

Population size can be found either by multiplying $D$ by the survey area or from a Horvitz-Thompson-like estimator (\cite[pp. 53-56]{thompson}, \cite[p. 23]{ADS}).

For the covariate models the population size may be estimated by:
\begin{equation}
\hat{N} = \sum_{i=1}^n \frac{1}{c_i \hat{p}_i},
\label{HT-ds-est}
\end{equation}
where $c_i$ is the \textit{coverage probability} (the probability of an individual lying within the surveyed area), and $\hat{p}_i$ is the probability of the $i\text{th}$ observation being detected given it is within the sampled area (\cite[p. 7 and 38]{ADS}). For the analyses presented here it is assumed that the probability of being in the sampled area is constant across the observations ($c_i=c$) and that:
\begin{equation*}
c=\frac{2wL}{A},
\end{equation*}
where $A$ is the area of the surveyed region. For non-covariate models, this simplifies to:
\begin{equation*}
\hat{N} =  \frac{n}{2 w L \hat{p}}A,
\end{equation*}
for line transects and
\begin{equation*}
\hat{N} =  \frac{n}{m \pi w^2 \hat{p}}A,
\end{equation*}
for point transects.

A standard summary statistic is the \textit{average detection probability} for an animal within the covered region, $\hat{P}_a$, which is given by:
\begin{equation*}
\hat{P}_a = n/\hat{N}.
\end{equation*}

\subsection{Plotting covariate models}
\label{ds-covplot}

\label{cor-8s12-1}In both this and the next chapter it will be necessary to plot the detection function for a covariate analysis. There are (at least) two ways of plotting the detection function in a covariate analysis. The first is as shown in figure \ref{ds-covarex}, where all but one of the covariates are held at a particular level and the other is varied (over say it's levels or quantiles). The other approach (which will be adopted from here) is to consider a covariate analysis as effectively fitting a detection function to each unique covariate combination, so averaging over the detection functions point-wise should give an indication of the overall shape of the detection function. In this case we evaluate the detection function at each unique covariate combination over distances ranging from 0 to $w$, then simply average their values at each distance to create an overall average detection function. To plot levels or quantiles, the requisite covariate is fixed to that level if there is only one covariate. If there is more than one covariate, the average performed as before fixing the covariate to its chosen value. This approach is useful here since the analyses are designed to highlight the possible shapes of the detection function, rather than to discover ecologically useful or informative covariates. 

From here on, when the phrase ``average detection function'' is used, this refers to a point-wise average of the possible detection functions (i.e. all of the unique values of the scale parameter) over the range $(0,w)$. When quantile/level plots are shown, it is the case that the quantile is fixed and the detection function is averaged over the other values. In each case the detection functions are normalised so that $g(0\vert\cdot)=1$.

\section{Other considerations}

\subsection{Line and point placement}
In \secref{ds-assumptions}, the placement of the lines and points above is said to be ``random'' with respect to the distribution of objects. In practice randomly placing and orientating lines can be expensive and time-consuming (in particular in shipboard surveys where one wishes to minimize off-effort time). The solution to this is simply randomly placing and orientating a grid of lines or points (\cite[p. 2]{IDS}). For shipboard surveys ``zigzag'' designs can be used to minimize off-effort time (\cite{strindberg04}). It is also important to ensure that transects to not run parallel to geographical features as doing this will incur bias. For example using roads as transects (as was done in the US breeding bird survey) leads to bias since animals may be compelled to move away from the road and toward neighbouring hedgerows (\cite[p. 18]{IDS}).

\subsection{Clusters}
If animals are observed in clusters (for example pods for whales or packs for wolves) then it might be more convenient to estimate the abundance of clusters and use them as the fundamental unit to estimate. The cluster size can also be estimated and the abundance of clusters ``multiplied up'' to give the overall abundance (\cite[p. 13]{IDS}). It is assumed throughout that individuals rather than clusters are being addressed but the method developed here should also work on clusters.

\subsection{Goodness of fit testing}
Although AIC is a good measure of relative fit of a model, some formal absolute measure of goodness of fit is also useful (the best of a bad lot is still bad). $\chi^2$ testing has been suggested (\cite[pp. 69-71]{IDS}), however the choice of interval is subjective. As a replacement, Kolmogorv-Smirnov and Cramer-von Mises tests (\cite[pp. 385-389]{ADS}) are suggested. Both are used to compare empirical to cumulative distribution functions (EDFs and CDFs, respectively). The Kolmogorov-Smirnov test uses the largest difference between the fitted CDF and the EDF as a test statistic, with the null hypothesis that the functions are the same. The Cramer-von Mises test has the same null hypothesis but the test statistic is instead based on the differences between the CDF and EDF over their entire range. The Kolmogorov-Smirnov test is used in the analyses in the next chapter.

\section{Monotonicity}
\label{intro-ds-mono}
One potential pitfall of both CDS and MCDS is that  it is possible to formulate models which are not physically realistic. In particular it is possible to create models for the detection function which are non-monotonic functions of distance. Data with a mode away from zero distance may occur when there has been heaping (when observers ``eyeball'' distances and round to convenient numbers like 5, 10, 20m etc, \cite[pp. 34-35]{IDS}), when objects move prior to observation. Fitting models to such ``bumps'' can cause bias in abundance estimates (\cite[p. 132]{IDS}). To get around this problem the software package Distance (\cite{distance-software})\label{cor-7s14-2} constrains the detection function to be monotonic. This is done by taking 10 equally spaced distances from $0$ to $w$ and checking that when the detection function is evaluated at each of these points they are less than the last ($g(x_i)\geq g(x_{i+1})$ for distances $x_1 \dots x_{10}$ where $x_1=0$). This is referred to as \textit{strong monotonicity}. Alternatively, \textit{weak monotonicity} may be enforced, where each point is checked only against the value of $g$ at the origin ($g(0)\geq g(x_i)$).

Constraining the shape of the detection function does not necessarily always lead to monotonic detection functions since the constraints can only be applied at a finite number of points. This can lead to constraints missing the non-monotonic points in the function. An example from \citeb{williams} is shown in the first plot of figure \ref{fig1}. Here a half-normal detection function with one second order cosine adjustment term was fitted to humpback whale sightings. In the MCDS case, \citeb{distance-software} do not constrain the detection function. The second and third panels in figure \ref{fig1} show a detection function when covariate data was included in the model. In this case for long-finned pilot whales (\cite{pike}) the Beaufort sea state was added as a covariate, the first plot shows that the detection function, when averaged over the covariate values, shows some non-monotonic behaviour. However, in the third panel shows the detection function the covariate is fixed to the values 1.5, 2 and 3 (from bottom to top) and here the non-monotonicity is particularly pronounced. 

Although constrained optimization is appealing, it is obviously always preferable to perform unconstrained optimization if possible. Using a class of functions to model the detection function which were both flexible and did not exhibit the undesirable property of non-monotonicity could offer a more  convincing and physically realistic alternative to the conventional way of performing distance analyses.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{mix/figs/figure1.pdf}
\caption{Two examples of detection functions which are not monotone. The first panel is data from humpback whale (reproduced from data in \citeb{williams}), a half-normal detection function with cosine adjustments provided the best fit to the data, even with constraints in place, the detection function is non-monotonic. The second and third panels show plots of a half-normal detection function with cosine adjustments for the long-finned pilot whale data taken from \citeb{pike}\label{cor-e14}. The second panel shows the average detection function (as described in \secref{ds-covplot}) and the third shows the detection function when the covariate values for the Beaufort sea \label{cor-e13}state are set to the values 1.5, 2 and 3 (from light to dark), showing that the non-monotonicity gets worse at higher levels of the covariate.\label{cor-r64}\label{cor-e15}}
\label{fig1}
\end{figure}

\label{cor-7s14}Aside from the monotonicity constraints that can be used in a CDS analysis in the Distance software, the problem of monotonicity has not been directly addressed in the literature to date. In practice, it is often the case that non-monotonicity is ignored in the hope that the resulting bias incurred is not too high. It is also possible that those analysing the data are unable to observe the non-monotonic nature of the detection function (for example in a complex covariate model). \citeb{innes2} avoid a non-monotonic detection function by aggressively truncating the data (i.e. reducing $w$); this approach is rather wasteful, particularly when most surveys are expensive to set up and run. Using all of the data, whilst maintaining monotonicity is certainly preferable.

\section{Mixture models}

Recent developments, particularly in mark-recapture (\cite{pledger2000}, \cite{dorazio03}, \cite{pledger2005} and \cite{morgan08}), have shown that mixture models can be an extremely useful and flexible method of modelling heterogeneity in biological populations. Their main utility has been in better accounting for between-individual heterogeneity which can cause severe bias if unmodelled (\cite{Link2003}). 

In distance sampling bias due to unmodelled heterogeneity is not severe unless the heterogeneity is extreme (\cite[pp. 389-392]{ADS}), provided that the detection at zero distance is certain and a flexible detection function model is used. So called \textit{pooling robustness} allows abundance estimates of the whole population to have low bias (although abundance estimates of subpopulations, e.g. males or females alone, may be biased). 

Mixture models offer the potential for flexible modelling of the detection function. They also have the appealing property that if the individual parts of the mixture model (the \textit{mixture components}) are each monotonic decreasing then a sum of these functions will also be monotonic decreasing. Each constituent component of the mixture model is simple but the combinations yield a great many possible shapes (see figure \ref{sim-detfcts}). Using such a set of models avoids constrained optimization.

%Covariates provide some information on those factors which may effect the detectability of individuals, modelling some of the heterogeneity in the detection process. Adjustment terms attempt to perform a similar task (though in an admittedly blunter fashion) by smoothing the function of distance in a semi-parametric way. A combination of the two approaches is possible although some have philosophical opposition to it (Jeff Laake, personal communication). One would like to both handle heterogeneity with both the data at hand (in the form of covariates) whilst also ``smoothing away'' residual effects of unobserved states in the population (while maintaining monotonicity).

Finally, the approach is also interesting for its own sake. There is no current literature on the use of mixture models as detection functions for distance sampling. Many detection function forms have been proposed (\cite{buckland92} and  \cite{gammadetfct}), each having their own merits and pitfalls. Therefore, there may be useful and unexplored properties of using a mixture model for the detection function that have not been previously considered.

A mixture model approach to distance sampling detection functions is developed further in the next chapter.

\section{Summary}
\label{cor-7s13}
Distance sampling is unlike many statistical methods, in that the quantity which we wish to find, abundance, is not given explicitly in the likelihood we wish to optimize. Instead we wish to find the parameters for the detection function, to then estimate of $\mu$, in order to estimate density (and hence the abundance). Full likelihood methods for distance sampling require that probabilistic models for the animal distribution be specified which may well be very tricky. CDS and MCDS specify probability models only for the distance parts of the data and then assume $\log$-normality for variances and confidence intervals for $\hat{N}$. Although this means that estimators do not have  properties such as asymptotic efficiency, they do avoid the specification of the animal distribution or, for MCDS, the joint distribution of the covariates and distances (see \cite[p. 6 and pp. 31-33]{ADS}). 

Distance sampling benefits from a relatively simple field procedure, a wealth of literature and easy to use software for analysis. Distance sampling has also been adapted for many different scenarios, including analysing data which were\label{cor-r1-7} not initially part of a survey (incidental data).

Other variants of distance sampling exist, for example mark-recapture distance sampling (MRDS, \cite{mrdspaper}) and spatial distance sampling models (\cite[chapter 4]{ADS}), although these are not addressed here.

In the next chapter, a mixture model approach for distance sampling detection functions will be proposed and applied to both simulated data and to case studies.
